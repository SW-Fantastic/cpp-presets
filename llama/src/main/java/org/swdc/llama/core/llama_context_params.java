// Targeted by JavaCPP version 1.5.10: DO NOT EDIT THIS FILE

package org.swdc.llama.core;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import org.swdc.llama.core.ggml.*;
import static org.swdc.llama.core.ggml.GGML.*;

import static org.swdc.llama.core.LLamaCore.*;


    // NOTE: changing the default values of parameters marked as [EXPERIMENTAL] may cause crashes or incorrect results in certain configurations
    //       https://github.com/ggml-org/llama.cpp/pull/7544
    @Properties(inherit = org.swdc.llama.config.LLamaConfigure.class)
public class llama_context_params extends Pointer {
        static { Loader.load(); }
        /** Default native constructor. */
        public llama_context_params() { super((Pointer)null); allocate(); }
        /** Native array allocator. Access with {@link Pointer#position(long)}. */
        public llama_context_params(long size) { super((Pointer)null); allocateArray(size); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public llama_context_params(Pointer p) { super(p); }
        private native void allocate();
        private native void allocateArray(long size);
        @Override public llama_context_params position(long position) {
            return (llama_context_params)super.position(position);
        }
        @Override public llama_context_params getPointer(long i) {
            return new llama_context_params((Pointer)this).offsetAddress(i);
        }
    
        public native @Cast("uint32_t") int n_ctx(); public native llama_context_params n_ctx(int setter);             // text context, 0 = from model
        public native @Cast("uint32_t") int n_batch(); public native llama_context_params n_batch(int setter);           // logical maximum batch size that can be submitted to llama_decode
        public native @Cast("uint32_t") int n_ubatch(); public native llama_context_params n_ubatch(int setter);          // physical maximum batch size
        public native @Cast("uint32_t") int n_seq_max(); public native llama_context_params n_seq_max(int setter);         // max number of sequences (i.e. distinct states for recurrent models)
        public native int n_threads(); public native llama_context_params n_threads(int setter);         // number of threads to use for generation
        public native int n_threads_batch(); public native llama_context_params n_threads_batch(int setter);   // number of threads to use for batch processing // RoPE scaling type, from `enum llama_rope_scaling_type`      // whether to pool (sum) embedding results by sequence id    // attention type to use for embeddings
        public native @Cast("llama_flash_attn_type") int flash_attn_type(); public native llama_context_params flash_attn_type(int setter);   // when to enable Flash Attention

        // ref: https://github.com/ggml-org/llama.cpp/pull/2054
        public native float rope_freq_base(); public native llama_context_params rope_freq_base(float setter);   // RoPE base frequency, 0 = from model
        public native float rope_freq_scale(); public native llama_context_params rope_freq_scale(float setter);  // RoPE frequency scaling factor, 0 = from model
        public native float yarn_ext_factor(); public native llama_context_params yarn_ext_factor(float setter);  // YaRN extrapolation mix factor, negative = from model
        public native float yarn_attn_factor(); public native llama_context_params yarn_attn_factor(float setter); // YaRN magnitude scaling factor
        public native float yarn_beta_fast(); public native llama_context_params yarn_beta_fast(float setter);   // YaRN low correction dim
        public native float yarn_beta_slow(); public native llama_context_params yarn_beta_slow(float setter);   // YaRN high correction dim
        public native @Cast("uint32_t") int yarn_orig_ctx(); public native llama_context_params yarn_orig_ctx(int setter);    // YaRN original context size
        public native float defrag_thold(); public native llama_context_params defrag_thold(float setter);     // [DEPRECATED] defragment the KV cache if holes/size > thold, <= 0 disabled (default)

        public native ggml_backend_sched_eval_callback cb_eval(); public native llama_context_params cb_eval(ggml_backend_sched_eval_callback setter);
        public native Pointer cb_eval_user_data(); public native llama_context_params cb_eval_user_data(Pointer setter);

        public native @Cast("ggml_type") int type_k(); public native llama_context_params type_k(int setter); // data type for K cache [EXPERIMENTAL]
        public native @Cast("ggml_type") int type_v(); public native llama_context_params type_v(int setter); // data type for V cache [EXPERIMENTAL]

        // Abort callback
        // if it returns true, execution of llama_decode() will be aborted
        // currently works only with CPU execution
        public native ggml_abort_callback abort_callback(); public native llama_context_params abort_callback(ggml_abort_callback setter);
        public native Pointer abort_callback_data(); public native llama_context_params abort_callback_data(Pointer setter);

        // Keep the booleans together and at the end of the struct to avoid misalignment during copy-by-value.
        public native @Cast("bool") boolean embeddings(); public native llama_context_params embeddings(boolean setter);  // if true, extract embeddings (together with logits)
        public native @Cast("bool") boolean offload_kqv(); public native llama_context_params offload_kqv(boolean setter); // offload the KQV ops (including the KV cache) to GPU
        public native @Cast("bool") boolean no_perf(); public native llama_context_params no_perf(boolean setter);     // measure performance timings
        public native @Cast("bool") boolean op_offload(); public native llama_context_params op_offload(boolean setter);  // offload host tensor operations to device
        public native @Cast("bool") boolean swa_full(); public native llama_context_params swa_full(boolean setter);    // use full-size SWA cache (https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
                          // NOTE: setting to false when n_seq_max > 1 can cause bad performance in some cases
                          //       ref: https://github.com/ggml-org/llama.cpp/pull/13845#issuecomment-2924800573
        public native @Cast("bool") boolean kv_unified(); public native llama_context_params kv_unified(boolean setter);  // use a unified buffer across the input sequences when computing the attention
                          // try to disable when n_seq_max > 1 for improved performance when the sequences do not share a large prefix
                          // ref: https://github.com/ggml-org/llama.cpp/pull/14363
    }
