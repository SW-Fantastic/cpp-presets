// Targeted by JavaCPP version 1.5.10: DO NOT EDIT THIS FILE

package org.swdc.llama.core;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import org.swdc.llama.core.ggml.*;
import static org.swdc.llama.core.ggml.GGML.*;

import static org.swdc.llama.core.LLamaCore.*;


    @Properties(inherit = org.swdc.llama.config.LLamaConfigure.class)
public class llama_model_params extends Pointer {
        static { Loader.load(); }
        /** Default native constructor. */
        public llama_model_params() { super((Pointer)null); allocate(); }
        /** Native array allocator. Access with {@link Pointer#position(long)}. */
        public llama_model_params(long size) { super((Pointer)null); allocateArray(size); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public llama_model_params(Pointer p) { super(p); }
        private native void allocate();
        private native void allocateArray(long size);
        @Override public llama_model_params position(long position) {
            return (llama_model_params)super.position(position);
        }
        @Override public llama_model_params getPointer(long i) {
            return new llama_model_params((Pointer)this).offsetAddress(i);
        }
    
        // NULL-terminated list of devices to use for offloading (if NULL, all available devices are used)
        public native ggml_backend_device devices(int i); public native llama_model_params devices(int i, ggml_backend_device setter);
        public native @Cast("ggml_backend_device**") PointerPointer devices(); public native llama_model_params devices(PointerPointer setter);

        // NULL-terminated list of buffer types to use for tensors that match a pattern
        public native @Const llama_model_tensor_buft_override tensor_buft_overrides(); public native llama_model_params tensor_buft_overrides(llama_model_tensor_buft_override setter);

        public native int n_gpu_layers(); public native llama_model_params n_gpu_layers(int setter); // number of layers to store in VRAM // how to split the model across multiple GPUs

        // the GPU that is used for the entire model when split_mode is LLAMA_SPLIT_MODE_NONE
        public native int main_gpu(); public native llama_model_params main_gpu(int setter);

        // proportion of the model (layers or rows) to offload to each GPU, size: llama_max_devices()
        public native @Const FloatPointer tensor_split(); public native llama_model_params tensor_split(FloatPointer setter);

        // Called with a progress value between 0.0 and 1.0. Pass NULL to disable.
        // If the provided progress_callback returns true, model loading continues.
        // If it returns false, model loading is immediately aborted.
        public native llama_progress_callback progress_callback(); public native llama_model_params progress_callback(llama_progress_callback setter);

        // context pointer passed to the progress callback
        public native Pointer progress_callback_user_data(); public native llama_model_params progress_callback_user_data(Pointer setter);

        // override key-value pairs of the model meta data
        public native @Const llama_model_kv_override kv_overrides(); public native llama_model_params kv_overrides(llama_model_kv_override setter);

        // Keep the booleans together to avoid misalignment during copy-by-value.
        public native @Cast("bool") boolean vocab_only(); public native llama_model_params vocab_only(boolean setter);      // only load the vocabulary, no weights
        public native @Cast("bool") boolean use_mmap(); public native llama_model_params use_mmap(boolean setter);        // use mmap if possible
        public native @Cast("bool") boolean use_mlock(); public native llama_model_params use_mlock(boolean setter);       // force system to keep model in RAM
        public native @Cast("bool") boolean check_tensors(); public native llama_model_params check_tensors(boolean setter);   // validate model tensor data
        public native @Cast("bool") boolean use_extra_bufts(); public native llama_model_params use_extra_bufts(boolean setter); // use extra buffer types (used for weight repacking)
    }
