// Targeted by JavaCPP version 1.5.10: DO NOT EDIT THIS FILE

package org.swdc.llama.core.ggml;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

public class GGML extends org.swdc.llama.config.GGMLConfigure {
    static { Loader.load(); }

// Parsed from ggml-alloc.h

// #pragma once

// #include "ggml.h"

// #ifdef  __cplusplus
// Targeting ggml_backend_buffer_type.java


// Targeting ggml_backend_buffer.java


// Targeting ggml_backend.java


// Targeting ggml_tallocr.java



public static native @ByVal ggml_tallocr ggml_tallocr_new(ggml_backend_buffer buffer);
public static native void ggml_tallocr_alloc(ggml_tallocr talloc, ggml_tensor tensor);
// Targeting ggml_gallocr.java



public static native ggml_gallocr ggml_gallocr_new(ggml_backend_buffer_type buft);
public static native ggml_gallocr ggml_gallocr_new_n(@Cast("ggml_backend_buffer_type**") PointerPointer bufts, int n_bufs);
public static native ggml_gallocr ggml_gallocr_new_n(@ByPtrPtr ggml_backend_buffer_type bufts, int n_bufs);
public static native void ggml_gallocr_free(ggml_gallocr galloc);

// pre-allocate buffers from a measure graph - does not allocate or modify the graph
// call with a worst-case graph to avoid buffer reallocations
// not strictly required for single buffer usage: ggml_gallocr_alloc_graph will reallocate the buffers automatically if needed
// returns false if the buffer allocation failed
public static native @Cast("bool") boolean ggml_gallocr_reserve(ggml_gallocr galloc, ggml_cgraph graph);
public static native @Cast("bool") boolean ggml_gallocr_reserve_n(
    ggml_gallocr galloc,
    ggml_cgraph graph,
    @Const IntPointer node_buffer_ids,
    @Const IntPointer leaf_buffer_ids);
public static native @Cast("bool") boolean ggml_gallocr_reserve_n(
    ggml_gallocr galloc,
    ggml_cgraph graph,
    @Const IntBuffer node_buffer_ids,
    @Const IntBuffer leaf_buffer_ids);
public static native @Cast("bool") boolean ggml_gallocr_reserve_n(
    ggml_gallocr galloc,
    ggml_cgraph graph,
    @Const int[] node_buffer_ids,
    @Const int[] leaf_buffer_ids);

// automatic reallocation if the topology changes when using a single buffer
// returns false if using multiple buffers and a re-allocation is needed (call ggml_gallocr_reserve_n first to set the node buffers)
public static native @Cast("bool") boolean ggml_gallocr_alloc_graph(ggml_gallocr galloc, ggml_cgraph graph);

public static native @Cast("size_t") long ggml_gallocr_get_buffer_size(ggml_gallocr galloc, int buffer_id);

// Utils
// Create a buffer and allocate all the tensors in a ggml_context
public static native ggml_backend_buffer ggml_backend_alloc_ctx_tensors_from_buft(ggml_context ctx, ggml_backend_buffer_type buft);
public static native ggml_backend_buffer ggml_backend_alloc_ctx_tensors(ggml_context ctx, ggml_backend backend);

// #ifdef  __cplusplus
// #endif


// Parsed from ggml.h

// #pragma once

//
// GGML Tensor Library
//
// This documentation is still a work in progress.
// If you wish some specific topics to be covered, feel free to drop a comment:
//
//   https://github.com/ggerganov/whisper.cpp/issues/40
//
// ## Overview
//
// This library implements:
//
//  - a set of tensor operations
//  - automatic differentiation
//  - basic optimization algorithms
//
// The aim of this library is to provide a minimalistic approach for various machine learning tasks. This includes,
// but is not limited to, the following:
//
//  - linear regression
//  - support vector machines
//  - neural networks
//
// The library allows the user to define a certain function using the available tensor operations. This function
// definition is represented internally via a computation graph. Each tensor operation in the function definition
// corresponds to a node in the graph. Having the computation graph defined, the user can choose to compute the
// function's value and/or its gradient with respect to the input variables. Optionally, the function can be optimized
// using one of the available optimization algorithms.
//
// For example, here we define the function: f(x) = a*x^2 + b
//
//   {
//       struct ggml_init_params params = {
//           .mem_size   = 16*1024*1024,
//           .mem_buffer = NULL,
//       };
//
//       // memory allocation happens here
//       struct ggml_context * ctx = ggml_init(params);
//
//       struct ggml_tensor * x = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//
//       ggml_set_param(ctx, x); // x is an input variable
//
//       struct ggml_tensor * a  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//       struct ggml_tensor * b  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//       struct ggml_tensor * x2 = ggml_mul(ctx, x, x);
//       struct ggml_tensor * f  = ggml_add(ctx, ggml_mul(ctx, a, x2), b);
//
//       ...
//   }
//
// Notice that the function definition above does not involve any actual computation. The computation is performed only
// when the user explicitly requests it. For example, to compute the function's value at x = 2.0:
//
//   {
//       ...
//
//       struct ggml_cgraph * gf = ggml_new_graph(ctx);
//       ggml_build_forward_expand(gf, f);
//
//       // set the input variable and parameter values
//       ggml_set_f32(x, 2.0f);
//       ggml_set_f32(a, 3.0f);
//       ggml_set_f32(b, 4.0f);
//
//       ggml_graph_compute_with_ctx(ctx, &gf, n_threads);
//
//       printf("f = %f\n", ggml_get_f32_1d(f, 0));
//
//       ...
//   }
//
// The actual computation is performed in the ggml_graph_compute() function.
//
// The ggml_new_tensor_...() functions create new tensors. They are allocated in the memory buffer provided to the
// ggml_init() function. You have to be careful not to exceed the memory buffer size. Therefore, you have to know
// in advance how much memory you need for your computation. Alternatively, you can allocate a large enough memory
// and after defining the computation graph, call the ggml_used_mem() function to find out how much memory was
// actually needed.
//
// The ggml_set_param() function marks a tensor as an input variable. This is used by the automatic
// differentiation and optimization algorithms.
//
// The described approach allows to define the function graph once and then compute its forward or backward graphs
// multiple times. All computations will use the same memory buffer allocated in the ggml_init() function. This way
// the user can avoid the memory allocation overhead at runtime.
//
// The library supports multi-dimensional tensors - up to 4 dimensions. The FP16 and FP32 data types are first class
// citizens, but in theory the library can be extended to support FP8 and integer data types.
//
// Each tensor operation produces a new tensor. Initially the library was envisioned to support only the use of unary
// and binary operations. Most of the available operations fall into one of these two categories. With time, it became
// clear that the library needs to support more complex operations. The way to support these operations is not clear
// yet, but a few examples are demonstrated in the following operations:
//
//   - ggml_permute()
//   - ggml_conv_1d_1s()
//   - ggml_conv_1d_2s()
//
// For each tensor operator, the library implements a forward and backward computation function. The forward function
// computes the output tensor value given the input tensor values. The backward function computes the adjoint of the
// input tensors given the adjoint of the output tensor. For a detailed explanation of what this means, take a
// calculus class, or watch the following video:
//
//   What is Automatic Differentiation?
//   https://www.youtube.com/watch?v=wG_nF1awSSY
//
//
// ## Tensor data (struct ggml_tensor)
//
// The tensors are stored in memory via the ggml_tensor struct. The structure provides information about the size of
// the tensor, the data type, and the memory buffer where the tensor data is stored. Additionally, it contains
// pointers to the "source" tensors - i.e. the tensors that were used to compute the current tensor. For example:
//
//   {
//       struct ggml_tensor * c = ggml_add(ctx, a, b);
//
//       assert(c->src[0] == a);
//       assert(c->src[1] == b);
//   }
//
// The multi-dimensional tensors are stored in row-major order. The ggml_tensor struct contains fields for the
// number of elements in each dimension ("ne") as well as the number of bytes ("nb", a.k.a. stride). This allows
// to store tensors that are not contiguous in memory, which is useful for operations such as transposition and
// permutation. All tensor operations have to take the stride into account and not assume that the tensor is
// contiguous in memory.
//
// The data of the tensor is accessed via the "data" pointer. For example:
//
//   {
//       const int nx = 2;
//       const int ny = 3;
//
//       struct ggml_tensor * a = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, nx, ny);
//
//       for (int y = 0; y < ny; y++) {
//           for (int x = 0; x < nx; x++) {
//               *(float *) ((char *) a->data + y*a->nb[1] + x*a->nb[0]) = x + y;
//           }
//       }
//
//       ...
//   }
//
// Alternatively, there are helper functions, such as ggml_get_f32_1d() and ggml_set_f32_1d() that can be used.
//
// ## The matrix multiplication operator (ggml_mul_mat)
//
// TODO
//
//
// ## Multi-threading
//
// TODO
//
//
// ## Overview of ggml.c
//
// TODO
//
//
// ## SIMD optimizations
//
// TODO
//
//
// ## Debugging ggml
//
// TODO
//
//

// #ifdef GGML_SHARED
// #    if defined(_WIN32) && !defined(__MINGW32__)
// #        ifdef GGML_BUILD
// #            define GGML_API __declspec(dllexport) extern
// #        else
// #            define GGML_API __declspec(dllimport) extern
// #        endif
// #    else
// #        define GGML_API __attribute__ ((visibility ("default"))) extern
// #    endif
// #else
// #    define GGML_API extern
// #endif

// TODO: support for clang
// #ifdef __GNUC__
// #    define GGML_DEPRECATED(func, hint) func __attribute__((deprecated(hint)))
// #elif defined(_MSC_VER)
// #    define GGML_DEPRECATED(func, hint) __declspec(deprecated(hint)) func
// #else
// #    define GGML_DEPRECATED(func, hint) func
// #endif

// #ifndef __GNUC__
// #    define GGML_ATTRIBUTE_FORMAT(...)
// #elif defined(__MINGW32__) && !defined(__clang__)
// #    define GGML_ATTRIBUTE_FORMAT(...) __attribute__((format(gnu_printf, __VA_ARGS__)))
// #else
// #    define GGML_ATTRIBUTE_FORMAT(...) __attribute__((format(printf, __VA_ARGS__)))
// #endif

// #include <stdbool.h>
// #include <stddef.h>
// #include <stdint.h>
// #include <stdio.h>

public static final int GGML_FILE_MAGIC =   0x67676d6c; // "ggml"
public static final int GGML_FILE_VERSION = 2;

public static final int GGML_QNT_VERSION =        2;    // bump this on quantization format changes
public static final int GGML_QNT_VERSION_FACTOR = 1000; // do not change this

public static final int GGML_MAX_DIMS =           4;
public static final int GGML_MAX_PARAMS =         2048;
public static final int GGML_MAX_SRC =            10;
public static final int GGML_MAX_N_THREADS =      512;
public static final int GGML_MAX_OP_PARAMS =      64;

// #ifndef GGML_MAX_NAME
public static final int GGML_MAX_NAME =        64;
// #endif

public static final int GGML_DEFAULT_N_THREADS =  4;
public static final int GGML_DEFAULT_GRAPH_SIZE = 2048;

// #if UINTPTR_MAX == 0xFFFFFFFF
    public static final int GGML_MEM_ALIGN = 4;
// #else
// #endif

public static final int GGML_EXIT_SUCCESS = 0;
public static final int GGML_EXIT_ABORTED = 1;

public static final int GGML_ROPE_TYPE_NEOX =   2;
public static final int GGML_ROPE_TYPE_MROPE =  8;
public static final int GGML_ROPE_TYPE_VISION = 24;

// #define GGML_UNUSED(x) (void)(x)

// #define GGML_PAD(x, n) (((x) + (n) - 1) & ~((n) - 1))

// #ifndef NDEBUG
// #   define GGML_UNREACHABLE() do { fprintf(stderr, "statement should be unreachable\n"); abort(); } while(0)
// #elif defined(__GNUC__)
// #   define GGML_UNREACHABLE() __builtin_unreachable()
// #elif defined(_MSC_VER)
// #   define GGML_UNREACHABLE() __assume(0)
// #else
// #   define GGML_UNREACHABLE() ((void) 0)
// #endif

// #ifdef __cplusplus
// #   define GGML_NORETURN [[noreturn]]
// #elif defined(_MSC_VER)
// #   define GGML_NORETURN __declspec(noreturn)
// #else
// #   define GGML_NORETURN _Noreturn
// #endif

// #define GGML_ABORT(...) ggml_abort(__FILE__, __LINE__, __VA_ARGS__)
// #define GGML_ASSERT(x) if (!(x)) GGML_ABORT("GGML_ASSERT(%s) failed", #x)

// used to copy the number of elements and stride in bytes of tensors into local variables.
// main purpose is to reduce code duplication and improve readability.
//
// example:
//
//    GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne);
//    GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb);
//
// #define GGML_TENSOR_LOCALS_1(type, prefix, pointer, array)
//     const type prefix##0 = (pointer)->array[0];
//     GGML_UNUSED(prefix##0);
// #define GGML_TENSOR_LOCALS_2(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_1    (type, prefix, pointer, array)
//     const type prefix##1 = (pointer)->array[1];
//     GGML_UNUSED(prefix##1);
// #define GGML_TENSOR_LOCALS_3(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_2    (type, prefix, pointer, array)
//     const type prefix##2 = (pointer)->array[2];
//     GGML_UNUSED(prefix##2);
// #define GGML_TENSOR_LOCALS(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_3  (type, prefix, pointer, array)
//     const type prefix##3 = (pointer)->array[3];
//     GGML_UNUSED(prefix##3);

// #define GGML_TENSOR_UNARY_OP_LOCALS
//     GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne)
//     GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb)
//     GGML_TENSOR_LOCALS(int64_t, ne,  dst,  ne)
//     GGML_TENSOR_LOCALS(size_t,  nb,  dst,  nb)

// #define GGML_TENSOR_BINARY_OP_LOCALS
//     GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne)
//     GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb)
//     GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne)
//     GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb)
//     GGML_TENSOR_LOCALS(int64_t, ne,  dst,  ne)
//     GGML_TENSOR_LOCALS(size_t,  nb,  dst,  nb)

// #define GGML_TENSOR_BINARY_OP_LOCALS01
//     GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne)
//     GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb)
//     GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne)
//     GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb)

// #ifdef  __cplusplus
// #endif

    /** enum ggml_status */
    public static final int
        GGML_STATUS_ALLOC_FAILED = -2,
        GGML_STATUS_FAILED = -1,
        GGML_STATUS_SUCCESS = 0,
        GGML_STATUS_ABORTED = 1;

    // get ggml_status name string
    public static native @Cast("const char*") BytePointer ggml_status_to_string(@Cast("ggml_status") int status);

    // ieee 754-2008 half-precision float16
    // todo: make this not an integral type
    public static native float ggml_fp16_to_fp32(@Cast("ggml_fp16_t") short arg0);
    public static native @Cast("ggml_fp16_t") short ggml_fp32_to_fp16(float arg0);
    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") ShortPointer arg0, FloatPointer arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") ShortBuffer arg0, FloatBuffer arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") short[] arg0, float[] arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_fp16_row(@Const FloatPointer arg0, @Cast("ggml_fp16_t*") ShortPointer arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_fp16_row(@Const FloatBuffer arg0, @Cast("ggml_fp16_t*") ShortBuffer arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_fp16_row(@Const float[] arg0, @Cast("ggml_fp16_t*") short[] arg1, @Cast("int64_t") long arg2);
// Targeting ggml_bf16_t.java


    public static native @ByVal ggml_bf16_t ggml_fp32_to_bf16(float arg0);
    public static native float ggml_bf16_to_fp32(@ByVal ggml_bf16_t arg0);  // consider just doing << 16
    public static native void ggml_bf16_to_fp32_row(@Const ggml_bf16_t arg0, FloatPointer arg1, @Cast("int64_t") long arg2);
    public static native void ggml_bf16_to_fp32_row(@Const ggml_bf16_t arg0, FloatBuffer arg1, @Cast("int64_t") long arg2);
    public static native void ggml_bf16_to_fp32_row(@Const ggml_bf16_t arg0, float[] arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_bf16_row_ref(@Const FloatPointer arg0, ggml_bf16_t arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_bf16_row_ref(@Const FloatBuffer arg0, ggml_bf16_t arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_bf16_row_ref(@Const float[] arg0, ggml_bf16_t arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_bf16_row(@Const FloatPointer arg0, ggml_bf16_t arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_bf16_row(@Const FloatBuffer arg0, ggml_bf16_t arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_bf16_row(@Const float[] arg0, ggml_bf16_t arg1, @Cast("int64_t") long arg2);
// Targeting ggml_object.java


// Targeting ggml_context.java


// Targeting ggml_cgraph.java



    // NOTE: always add types at the end of the enum to keep backward compatibility
    /** enum ggml_type */
    public static final int
        GGML_TYPE_F32     = 0,
        GGML_TYPE_F16     = 1,
        GGML_TYPE_Q4_0    = 2,
        GGML_TYPE_Q4_1    = 3,
        // GGML_TYPE_Q4_2 = 4, support has been removed
        // GGML_TYPE_Q4_3 = 5, support has been removed
        GGML_TYPE_Q5_0    = 6,
        GGML_TYPE_Q5_1    = 7,
        GGML_TYPE_Q8_0    = 8,
        GGML_TYPE_Q8_1    = 9,
        GGML_TYPE_Q2_K    = 10,
        GGML_TYPE_Q3_K    = 11,
        GGML_TYPE_Q4_K    = 12,
        GGML_TYPE_Q5_K    = 13,
        GGML_TYPE_Q6_K    = 14,
        GGML_TYPE_Q8_K    = 15,
        GGML_TYPE_IQ2_XXS = 16,
        GGML_TYPE_IQ2_XS  = 17,
        GGML_TYPE_IQ3_XXS = 18,
        GGML_TYPE_IQ1_S   = 19,
        GGML_TYPE_IQ4_NL  = 20,
        GGML_TYPE_IQ3_S   = 21,
        GGML_TYPE_IQ2_S   = 22,
        GGML_TYPE_IQ4_XS  = 23,
        GGML_TYPE_I8      = 24,
        GGML_TYPE_I16     = 25,
        GGML_TYPE_I32     = 26,
        GGML_TYPE_I64     = 27,
        GGML_TYPE_F64     = 28,
        GGML_TYPE_IQ1_M   = 29,
        GGML_TYPE_BF16    = 30,
        // GGML_TYPE_Q4_0_4_4 = 31, support has been removed from gguf files
        // GGML_TYPE_Q4_0_4_8 = 32,
        // GGML_TYPE_Q4_0_8_8 = 33,
        GGML_TYPE_TQ1_0   = 34,
        GGML_TYPE_TQ2_0   = 35,
        // GGML_TYPE_IQ4_NL_4_4 = 36,
        // GGML_TYPE_IQ4_NL_4_8 = 37,
        // GGML_TYPE_IQ4_NL_8_8 = 38,
        GGML_TYPE_COUNT   = 39;

    // precision
    /** enum ggml_prec */
    public static final int
        GGML_PREC_DEFAULT = 0,
        GGML_PREC_F32 = 1;

    // model file types
    /** enum ggml_ftype */
    public static final int
        GGML_FTYPE_UNKNOWN        = -1,
        GGML_FTYPE_ALL_F32        = 0,
        GGML_FTYPE_MOSTLY_F16     = 1,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_0    = 2,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_1    = 3,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4, // tok_embeddings.weight and output.weight are F16
        GGML_FTYPE_MOSTLY_Q8_0    = 7,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_0    = 8,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_1    = 9,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q2_K    = 10, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q3_K    = 11, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_K    = 12, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_K    = 13, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q6_K    = 14, // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ2_XXS = 15, // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ2_XS  = 16, // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ3_XXS = 17, // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ1_S   = 18, // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ4_NL  = 19, // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ3_S   = 20, // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ2_S   = 21, // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ4_XS  = 22, // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ1_M   = 23, // except 1d tensors
        GGML_FTYPE_MOSTLY_BF16    = 24; // except 1d tensors

    // available tensor operations:
    /** enum ggml_op */
    public static final int
        GGML_OP_NONE = 0,

        GGML_OP_DUP = 1,
        GGML_OP_ADD = 2,
        GGML_OP_ADD1 = 3,
        GGML_OP_ACC = 4,
        GGML_OP_SUB = 5,
        GGML_OP_MUL = 6,
        GGML_OP_DIV = 7,
        GGML_OP_SQR = 8,
        GGML_OP_SQRT = 9,
        GGML_OP_LOG = 10,
        GGML_OP_SIN = 11,
        GGML_OP_COS = 12,
        GGML_OP_SUM = 13,
        GGML_OP_SUM_ROWS = 14,
        GGML_OP_MEAN = 15,
        GGML_OP_ARGMAX = 16,
        GGML_OP_COUNT_EQUAL = 17,
        GGML_OP_REPEAT = 18,
        GGML_OP_REPEAT_BACK = 19,
        GGML_OP_CONCAT = 20,
        GGML_OP_SILU_BACK = 21,
        GGML_OP_NORM = 22, // normalize
        GGML_OP_RMS_NORM = 23,
        GGML_OP_RMS_NORM_BACK = 24,
        GGML_OP_GROUP_NORM = 25,

        GGML_OP_MUL_MAT = 26,
        GGML_OP_MUL_MAT_ID = 27,
        GGML_OP_OUT_PROD = 28,

        GGML_OP_SCALE = 29,
        GGML_OP_SET = 30,
        GGML_OP_CPY = 31,
        GGML_OP_CONT = 32,
        GGML_OP_RESHAPE = 33,
        GGML_OP_VIEW = 34,
        GGML_OP_PERMUTE = 35,
        GGML_OP_TRANSPOSE = 36,
        GGML_OP_GET_ROWS = 37,
        GGML_OP_GET_ROWS_BACK = 38,
        GGML_OP_DIAG = 39,
        GGML_OP_DIAG_MASK_INF = 40,
        GGML_OP_DIAG_MASK_ZERO = 41,
        GGML_OP_SOFT_MAX = 42,
        GGML_OP_SOFT_MAX_BACK = 43,
        GGML_OP_ROPE = 44,
        GGML_OP_ROPE_BACK = 45,
        GGML_OP_CLAMP = 46,
        GGML_OP_CONV_TRANSPOSE_1D = 47,
        GGML_OP_IM2COL = 48,
        GGML_OP_IM2COL_BACK = 49,
        GGML_OP_CONV_TRANSPOSE_2D = 50,
        GGML_OP_POOL_1D = 51,
        GGML_OP_POOL_2D = 52,
        GGML_OP_POOL_2D_BACK = 53,
        GGML_OP_UPSCALE = 54, // nearest interpolate
        GGML_OP_PAD = 55,
        GGML_OP_PAD_REFLECT_1D = 56,
        GGML_OP_ARANGE = 57,
        GGML_OP_TIMESTEP_EMBEDDING = 58,
        GGML_OP_ARGSORT = 59,
        GGML_OP_LEAKY_RELU = 60,

        GGML_OP_FLASH_ATTN_EXT = 61,
        GGML_OP_FLASH_ATTN_BACK = 62,
        GGML_OP_SSM_CONV = 63,
        GGML_OP_SSM_SCAN = 64,
        GGML_OP_WIN_PART = 65,
        GGML_OP_WIN_UNPART = 66,
        GGML_OP_GET_REL_POS = 67,
        GGML_OP_ADD_REL_POS = 68,
        GGML_OP_RWKV_WKV6 = 69,
        GGML_OP_GATED_LINEAR_ATTN = 70,

        GGML_OP_UNARY = 71,

        GGML_OP_MAP_UNARY = 72,
        GGML_OP_MAP_BINARY = 73,

        GGML_OP_MAP_CUSTOM1_F32 = 74,
        GGML_OP_MAP_CUSTOM2_F32 = 75,
        GGML_OP_MAP_CUSTOM3_F32 = 76,

        GGML_OP_MAP_CUSTOM1 = 77,
        GGML_OP_MAP_CUSTOM2 = 78,
        GGML_OP_MAP_CUSTOM3 = 79,

        GGML_OP_CROSS_ENTROPY_LOSS = 80,
        GGML_OP_CROSS_ENTROPY_LOSS_BACK = 81,
        GGML_OP_OPT_STEP_ADAMW = 82,

        GGML_OP_COUNT = 83;

    /** enum ggml_unary_op */
    public static final int
        GGML_UNARY_OP_ABS = 0,
        GGML_UNARY_OP_SGN = 1,
        GGML_UNARY_OP_NEG = 2,
        GGML_UNARY_OP_STEP = 3,
        GGML_UNARY_OP_TANH = 4,
        GGML_UNARY_OP_ELU = 5,
        GGML_UNARY_OP_RELU = 6,
        GGML_UNARY_OP_SIGMOID = 7,
        GGML_UNARY_OP_GELU = 8,
        GGML_UNARY_OP_GELU_QUICK = 9,
        GGML_UNARY_OP_SILU = 10,
        GGML_UNARY_OP_HARDSWISH = 11,
        GGML_UNARY_OP_HARDSIGMOID = 12,
        GGML_UNARY_OP_EXP = 13,

        GGML_UNARY_OP_COUNT = 14;

    /** enum ggml_object_type */
    public static final int
        GGML_OBJECT_TYPE_TENSOR = 0,
        GGML_OBJECT_TYPE_GRAPH = 1,
        GGML_OBJECT_TYPE_WORK_BUFFER = 2;

    /** enum ggml_log_level */
    public static final int
        GGML_LOG_LEVEL_NONE  = 0,
        GGML_LOG_LEVEL_DEBUG = 1,
        GGML_LOG_LEVEL_INFO  = 2,
        GGML_LOG_LEVEL_WARN  = 3,
        GGML_LOG_LEVEL_ERROR = 4,
        GGML_LOG_LEVEL_CONT  = 5; // continue previous log

    // this tensor...
    /** enum ggml_tensor_flag */
    public static final int
        GGML_TENSOR_FLAG_INPUT  = 1, // ...is an input for the GGML compute graph
        GGML_TENSOR_FLAG_OUTPUT = 2, // ...is an output for the GGML compute graph
        GGML_TENSOR_FLAG_PARAM  = 4, // ...contains trainable parameters
        GGML_TENSOR_FLAG_LOSS   = 8; // ...defines loss for numerical optimization (multiple loss tensors add up)
// Targeting ggml_init_params.java


// Targeting ggml_tensor.java



    @MemberGetter public static native @Cast("const size_t") long GGML_TENSOR_SIZE();
    public static final long GGML_TENSOR_SIZE = GGML_TENSOR_SIZE();
// Targeting ggml_abort_callback.java




    //
    // GUID
    //

    // GUID types

    public static native @Cast("bool") boolean ggml_guid_matches(@Cast("ggml_guid_t") byte guid_a, @Cast("ggml_guid_t") byte guid_b);

    // misc

    public static native void ggml_time_init(); // call this once at the beginning of the program
    public static native @Cast("int64_t") long ggml_time_ms();
    public static native @Cast("int64_t") long ggml_time_us();
    public static native @Cast("int64_t") long ggml_cycles();
    public static native @Cast("int64_t") long ggml_cycles_per_ms();

    // accepts a UTF-8 path, even on Windows
    public static native @Cast("FILE*") Pointer ggml_fopen(@Cast("const char*") BytePointer fname, @Cast("const char*") BytePointer mode);
    public static native @Cast("FILE*") Pointer ggml_fopen(String fname, String mode);

    public static native void ggml_print_object(@Const ggml_object obj);
    public static native void ggml_print_objects(@Const ggml_context ctx);

    public static native @Cast("int64_t") long ggml_nelements(@Const ggml_tensor tensor);
    public static native @Cast("int64_t") long ggml_nrows(@Const ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_nbytes(@Const ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_nbytes_pad(@Const ggml_tensor tensor); // same as ggml_nbytes() but padded to GGML_MEM_ALIGN

    public static native @Cast("int64_t") long ggml_blck_size(@Cast("ggml_type") int type);
    public static native @Cast("size_t") long ggml_type_size(@Cast("ggml_type") int type);             // size in bytes for all elements in a block
    public static native @Cast("size_t") long ggml_row_size(@Cast("ggml_type") int type, @Cast("int64_t") long ne); // size in bytes for all elements in a row

    public static native double ggml_type_sizef(@Cast("ggml_type") int type);

    public static native @Cast("const char*") BytePointer ggml_type_name(@Cast("ggml_type") int type);
    public static native @Cast("const char*") BytePointer ggml_op_name(@Cast("ggml_op") int op);
    public static native @Cast("const char*") BytePointer ggml_op_symbol(@Cast("ggml_op") int op);

    public static native @Cast("const char*") BytePointer ggml_unary_op_name(@Cast("ggml_unary_op") int op);
    public static native @Cast("const char*") BytePointer ggml_op_desc(@Const ggml_tensor t); // unary or op name

    public static native @Cast("size_t") long ggml_element_size(@Const ggml_tensor tensor);

    public static native @Cast("bool") boolean ggml_is_quantized(@Cast("ggml_type") int type);

    // TODO: temporary until model loading of ggml examples is refactored
    public static native @Cast("ggml_type") int ggml_ftype_to_ggml_type(@Cast("ggml_ftype") int ftype);

    public static native @Cast("bool") boolean ggml_is_transposed(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_permuted(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_empty(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_scalar(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_vector(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_matrix(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_3d(@Const ggml_tensor tensor);
    public static native int ggml_n_dims(@Const ggml_tensor tensor); // returns 1 for scalars

    public static native @Cast("bool") boolean ggml_is_contiguous(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_contiguous_0(@Const ggml_tensor tensor); // same as ggml_is_contiguous()
    public static native @Cast("bool") boolean ggml_is_contiguous_1(@Const ggml_tensor tensor); // contiguous for dims >= 1
    public static native @Cast("bool") boolean ggml_is_contiguous_2(@Const ggml_tensor tensor); // contiguous for dims >= 2

    public static native @Cast("bool") boolean ggml_are_same_shape(@Const ggml_tensor t0, @Const ggml_tensor t1);
    public static native @Cast("bool") boolean ggml_are_same_stride(@Const ggml_tensor t0, @Const ggml_tensor t1);

    public static native @Cast("bool") boolean ggml_can_repeat(@Const ggml_tensor t0, @Const ggml_tensor t1);

    // use this to compute the memory overhead of a tensor
    public static native @Cast("size_t") long ggml_tensor_overhead();

    public static native @Cast("bool") boolean ggml_validate_row_data(@Cast("ggml_type") int type, @Const Pointer data, @Cast("size_t") long nbytes);

    // main

    public static native ggml_context ggml_init(@ByVal ggml_init_params params);
    public static native void ggml_reset(ggml_context ctx);
    public static native void ggml_free(ggml_context ctx);

    public static native @Cast("size_t") long ggml_used_mem(@Const ggml_context ctx);

    public static native @Cast("bool") boolean ggml_get_no_alloc(ggml_context ctx);
    public static native void ggml_set_no_alloc(ggml_context ctx, @Cast("bool") boolean no_alloc);

    public static native Pointer ggml_get_mem_buffer(@Const ggml_context ctx);
    public static native @Cast("size_t") long ggml_get_mem_size(@Const ggml_context ctx);
    public static native @Cast("size_t") long ggml_get_max_tensor_size(@Const ggml_context ctx);

    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") LongPointer ne);
    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") LongBuffer ne);
    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") long[] ne);

    public static native ggml_tensor ggml_new_tensor_1d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0);

    public static native ggml_tensor ggml_new_tensor_2d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1);

    public static native ggml_tensor ggml_new_tensor_3d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2);

    public static native ggml_tensor ggml_new_tensor_4d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3);

    public static native Pointer ggml_new_buffer(ggml_context ctx, @Cast("size_t") long nbytes);

    public static native ggml_tensor ggml_dup_tensor(ggml_context ctx, @Const ggml_tensor src);
    public static native ggml_tensor ggml_view_tensor(ggml_context ctx, ggml_tensor src);

    // Context tensor enumeration and lookup
    public static native ggml_tensor ggml_get_first_tensor(@Const ggml_context ctx);
    public static native ggml_tensor ggml_get_next_tensor(@Const ggml_context ctx, ggml_tensor tensor);
    public static native ggml_tensor ggml_get_tensor(ggml_context ctx, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_get_tensor(ggml_context ctx, String name);

    // Converts a flat index into coordinates
    public static native void ggml_unravel_index(@Const ggml_tensor tensor, @Cast("int64_t") long i, @Cast("int64_t*") LongPointer i0, @Cast("int64_t*") LongPointer i1, @Cast("int64_t*") LongPointer i2, @Cast("int64_t*") LongPointer i3);
    public static native void ggml_unravel_index(@Const ggml_tensor tensor, @Cast("int64_t") long i, @Cast("int64_t*") LongBuffer i0, @Cast("int64_t*") LongBuffer i1, @Cast("int64_t*") LongBuffer i2, @Cast("int64_t*") LongBuffer i3);
    public static native void ggml_unravel_index(@Const ggml_tensor tensor, @Cast("int64_t") long i, @Cast("int64_t*") long[] i0, @Cast("int64_t*") long[] i1, @Cast("int64_t*") long[] i2, @Cast("int64_t*") long[] i3);

    public static native @Cast("ggml_unary_op") int ggml_get_unary_op(@Const ggml_tensor tensor);

    public static native Pointer ggml_get_data(@Const ggml_tensor tensor);
    public static native FloatPointer ggml_get_data_f32(@Const ggml_tensor tensor);

    public static native @Cast("const char*") BytePointer ggml_get_name(@Const ggml_tensor tensor);
    public static native ggml_tensor ggml_set_name(      ggml_tensor tensor, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_set_name(      ggml_tensor tensor, String name);
    public static native ggml_tensor ggml_format_name(      ggml_tensor tensor, @Cast("const char*") BytePointer fmt);
    public static native ggml_tensor ggml_format_name(      ggml_tensor tensor, String fmt);

    // Tensor flags
    public static native void ggml_set_input(ggml_tensor tensor);
    public static native void ggml_set_output(ggml_tensor tensor);
    public static native void ggml_set_param(ggml_context ctx, ggml_tensor tensor);
    public static native void ggml_set_loss(ggml_tensor tensor);

    //
    // operations on tensors with backpropagation
    //

    public static native ggml_tensor ggml_dup(
                ggml_context ctx,
                ggml_tensor a);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_dup_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_add(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add_cast(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("ggml_type") int type);

    public static native ggml_tensor ggml_add1(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add1_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // dst = a
    // view(dst, nb1, nb2, nb3, offset) += b
    // return dst
    public static native ggml_tensor ggml_acc(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_acc_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_sub(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_sub_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_mul(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_mul_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_div(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_div_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_sqr(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqr_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqrt(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqrt_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_log(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_log_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sin(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sin_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_cos(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_cos_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // return scalar
    public static native ggml_tensor ggml_sum(
                ggml_context ctx,
                ggml_tensor a);

    // sums along rows, with input shape [a,b,c,d] return shape [1,b,c,d]
    public static native ggml_tensor ggml_sum_rows(
                ggml_context ctx,
                ggml_tensor a);

    // mean along rows
    public static native ggml_tensor ggml_mean(
                ggml_context ctx,
                ggml_tensor a);

    // argmax along rows
    public static native ggml_tensor ggml_argmax(
                ggml_context ctx,
                ggml_tensor a);

    // count number of equal elements in a and b
    public static native ggml_tensor ggml_count_equal(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // if a is the same shape as b, and a is not parameter, return a
    // otherwise, return a new tensor: repeat(a) to fit in b
    public static native ggml_tensor ggml_repeat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // sums repetitions in a into shape of b
    public static native ggml_tensor ggml_repeat_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // concat a and b along dim
    // used in stable-diffusion
    public static native ggml_tensor ggml_concat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int dim);

    public static native ggml_tensor ggml_abs(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_abs_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sgn(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sgn_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_neg(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_neg_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_step(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_step_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_tanh(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_tanh_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_elu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_elu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_relu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_leaky_relu(
                ggml_context ctx,
                ggml_tensor a, float negative_slope, @Cast("bool") boolean inplace);

    public static native ggml_tensor ggml_relu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sigmoid(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sigmoid_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_quick(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_quick_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_silu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_silu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // a - x
    // b - dy
    public static native ggml_tensor ggml_silu_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // hardswish(x) = x * relu6(x + 3) / 6
    public static native ggml_tensor ggml_hardswish(
                ggml_context ctx,
                ggml_tensor a);

    // hardsigmoid(x) = relu6(x + 3) / 6
    public static native ggml_tensor ggml_hardsigmoid(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_exp(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_exp_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // normalize along rows
    public static native ggml_tensor ggml_norm(
                ggml_context ctx,
                ggml_tensor a,
                float eps);

    public static native ggml_tensor ggml_norm_inplace(
                ggml_context ctx,
                ggml_tensor a,
                float eps);

    public static native ggml_tensor ggml_rms_norm(
                ggml_context ctx,
                ggml_tensor a,
                float eps);

    public static native ggml_tensor ggml_rms_norm_inplace(
                ggml_context ctx,
                ggml_tensor a,
                float eps);

    // group normalize along ne0*ne1*n_groups
    // used in stable-diffusion
    public static native ggml_tensor ggml_group_norm(
                ggml_context ctx,
                ggml_tensor a,
                int n_groups,
                float eps);

    public static native ggml_tensor ggml_group_norm_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_groups,
                float eps);

    // a - x
    // b - dy
    public static native ggml_tensor ggml_rms_norm_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                float eps);

    // A: k columns, n rows => [ne03, ne02, n, k]
    // B: k columns, m rows  (i.e. we transpose it internally) => [ne03 * x, ne02 * y, m, k]
    // result is n columns, m rows => [ne03 * x, ne02 * y, m, n]
    public static native ggml_tensor ggml_mul_mat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // change the precision of a matrix multiplication
    // set to GGML_PREC_F32 for higher precision (useful for phi-2)
    public static native void ggml_mul_mat_set_prec(
                ggml_tensor a,
                @Cast("ggml_prec") int prec);

    // indirect matrix multiplication
    public static native ggml_tensor ggml_mul_mat_id(
                ggml_context ctx,
                ggml_tensor as,
                ggml_tensor b,
                ggml_tensor ids);

    // A: m columns, n rows,
    // B: p columns, n rows,
    // result is m columns, p rows
    public static native ggml_tensor ggml_out_prod(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    //
    // operations on tensors without backpropagation
    //

    public static native ggml_tensor ggml_scale(
                ggml_context ctx,
                ggml_tensor a,
                float s);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_scale_inplace(
                ggml_context ctx,
                ggml_tensor a,
                float s);

    // b -> view(a,offset,nb1,nb2,3), return modified a
    public static native ggml_tensor ggml_set(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset); // in bytes

    // b -> view(a,offset,nb1,nb2,3), return view(a)
    public static native ggml_tensor ggml_set_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset); // in bytes

    public static native ggml_tensor ggml_set_1d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long offset); // in bytes

    public static native ggml_tensor ggml_set_1d_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long offset); // in bytes

    // b -> view(a,offset,nb1,nb2,3), return modified a
    public static native ggml_tensor ggml_set_2d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset); // in bytes

    // b -> view(a,offset,nb1,nb2,3), return view(a)
    public static native ggml_tensor ggml_set_2d_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset); // in bytes

    // a -> b, return view(b)
    public static native ggml_tensor ggml_cpy(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_cast(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("ggml_type") int type);

    // make contiguous
    public static native ggml_tensor ggml_cont(
                ggml_context ctx,
                ggml_tensor a);

    // make contiguous, with new shape
    public static native ggml_tensor ggml_cont_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0);

    public static native ggml_tensor ggml_cont_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1);

    public static native ggml_tensor ggml_cont_3d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2);

    public static native ggml_tensor ggml_cont_4d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3);

    // return view(a), b specifies the new shape
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // return view(a)
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0);

    public static native ggml_tensor ggml_reshape_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1);

    // return view(a)
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape_3d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2);

    public static native ggml_tensor ggml_reshape_4d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3);

    // offset in bytes
    public static native ggml_tensor ggml_view_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_3d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_4d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_permute(
                ggml_context ctx,
                ggml_tensor a,
                int axis0,
                int axis1,
                int axis2,
                int axis3);

    // alias for ggml_permute(ctx, a, 1, 0, 2, 3)
    public static native ggml_tensor ggml_transpose(
                ggml_context ctx,
                ggml_tensor a);

    // supports 3D: a->ne[2] == b->ne[1]
    public static native ggml_tensor ggml_get_rows(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b); // row indices

    public static native ggml_tensor ggml_get_rows_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c); // data for ggml_get_rows, only used for its shape

    public static native ggml_tensor ggml_diag(
            ggml_context ctx,
            ggml_tensor a);

    // set elements above the diagonal to -INF
    public static native ggml_tensor ggml_diag_mask_inf(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_diag_mask_inf_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // set elements above the diagonal to 0
    public static native ggml_tensor ggml_diag_mask_zero(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_diag_mask_zero_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    public static native ggml_tensor ggml_soft_max(
                ggml_context ctx,
                ggml_tensor a);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_soft_max_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // fused soft_max(a*scale + mask*(ALiBi slope))
    // mask is optional
    // max_bias = 0.0f for no ALiBi
    public static native ggml_tensor ggml_soft_max_ext(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor mask,
                float scale,
                float max_bias);

    public static native ggml_tensor ggml_soft_max_ext_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                float scale,
                float max_bias);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_soft_max_ext_back_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                float scale,
                float max_bias);

    // rotary position embedding
    // if (mode & 1) - skip n_past elements (NOT SUPPORTED)
    // if (mode & GGML_ROPE_TYPE_NEOX) - GPT-NeoX style
    //
    // b is an int32 vector with size a->ne[2], it contains the positions
    public static native ggml_tensor ggml_rope(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int n_dims,
                int mode);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_rope_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int n_dims,
                int mode);

    // custom RoPE
    // c is freq factors (e.g. phi3-128k), (optional)
    public static native ggml_tensor ggml_rope_ext(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);

    public static native ggml_tensor ggml_rope_multi(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                IntPointer sections,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);
    public static native ggml_tensor ggml_rope_multi(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                IntBuffer sections,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);
    public static native ggml_tensor ggml_rope_multi(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                int[] sections,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_rope_ext_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);

    public static native ggml_tensor ggml_rope_custom(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int n_dims,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);

    public static native ggml_tensor ggml_rope_custom_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int n_dims,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);

    // compute correction dims for YaRN RoPE scaling
    public static native void ggml_rope_yarn_corr_dims(
            int n_dims, int n_ctx_orig, float freq_base, float beta_fast, float beta_slow, FloatPointer dims);
    public static native void ggml_rope_yarn_corr_dims(
            int n_dims, int n_ctx_orig, float freq_base, float beta_fast, float beta_slow, FloatBuffer dims);
    public static native void ggml_rope_yarn_corr_dims(
            int n_dims, int n_ctx_orig, float freq_base, float beta_fast, float beta_slow, float[] dims);

    // rotary position embedding backward, i.e compute dx from dy
    // a - dy
    public static native ggml_tensor ggml_rope_ext_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);

    public static native ggml_tensor ggml_rope_multi_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                IntPointer sections,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);
    public static native ggml_tensor ggml_rope_multi_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                IntBuffer sections,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);
    public static native ggml_tensor ggml_rope_multi_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                int[] sections,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);


    // clamp
    // in-place, returns view(a)
    public static native ggml_tensor ggml_clamp(
                ggml_context ctx,
                ggml_tensor a,
                float min,
                float max);

    // im2col
    // converts data into a format that effectively results in a convolution when combined with matrix multiplication
    public static native ggml_tensor ggml_im2col(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int s1,
                int p0,
                int p1,
                int d0,
                int d1,
                @Cast("bool") boolean is_2D,
                @Cast("ggml_type") int dst_type);

    public static native ggml_tensor ggml_im2col_back(
            ggml_context ctx,
            ggml_tensor a,
            ggml_tensor b,
            @Cast("int64_t*") LongPointer ne,
            int s0,
            int s1,
            int p0,
            int p1,
            int d0,
            int d1,
            @Cast("bool") boolean is_2D);
    public static native ggml_tensor ggml_im2col_back(
            ggml_context ctx,
            ggml_tensor a,
            ggml_tensor b,
            @Cast("int64_t*") LongBuffer ne,
            int s0,
            int s1,
            int p0,
            int p1,
            int d0,
            int d1,
            @Cast("bool") boolean is_2D);
    public static native ggml_tensor ggml_im2col_back(
            ggml_context ctx,
            ggml_tensor a,
            ggml_tensor b,
            @Cast("int64_t*") long[] ne,
            int s0,
            int s1,
            int p0,
            int p1,
            int d0,
            int d1,
            @Cast("bool") boolean is_2D);

    public static native ggml_tensor ggml_conv_1d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int p0,
                int d0); // dilation

    // conv_1d with padding = half
    // alias for ggml_conv_1d(a, b, s, a->ne[0]/2, d)
    public static native ggml_tensor ggml_conv_1d_ph(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s,
                int d); // dilation

    // depthwise
    // TODO: this is very likely wrong for some cases! - needs more testing
    public static native ggml_tensor ggml_conv_1d_dw(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int p0,
                int d0); // dilation

    public static native ggml_tensor ggml_conv_1d_dw_ph(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int d0); // dilation

    public static native ggml_tensor ggml_conv_transpose_1d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int p0,
                int d0); // dilation

    public static native ggml_tensor ggml_conv_2d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int s1,
                int p0,
                int p1,
                int d0,
                int d1); // dilation dimension 1

    // kernel size is a->ne[0] x a->ne[1]
    // stride is equal to kernel size
    // padding is zero
    // example:
    // a:     16   16    3  768
    // b:   1024 1024    3    1
    // res:   64   64  768    1
    // used in sam
    public static native ggml_tensor ggml_conv_2d_sk_p0(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // kernel size is a->ne[0] x a->ne[1]
    // stride is 1
    // padding is half
    // example:
    // a:      3    3    256  256
    // b:     64   64    256    1
    // res:   64   64    256    1
    // used in sam
    public static native ggml_tensor ggml_conv_2d_s1_ph(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // depthwise
    public static native ggml_tensor ggml_conv_2d_dw(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int s1,
                int p0,
                int p1,
                int d0,
                int d1); // dilation dimension 1

    public static native ggml_tensor ggml_conv_transpose_2d_p0(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int stride);

    /** enum ggml_op_pool */
    public static final int
        GGML_OP_POOL_MAX = 0,
        GGML_OP_POOL_AVG = 1,
        GGML_OP_POOL_COUNT = 2;

    public static native ggml_tensor ggml_pool_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("ggml_op_pool") int op,
                int k0,
                int s0,
                int p0); // padding

    // the result will have 2*p0 padding for the first dimension
    // and 2*p1 padding for the second dimension
    public static native ggml_tensor ggml_pool_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("ggml_op_pool") int op,
                int k0,
                int k1,
                int s0,
                int s1,
                float p0,
                float p1);

    public static native ggml_tensor ggml_pool_2d_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor af,
                @Cast("ggml_op_pool") int op,
                int k0,
                int k1,
                int s0,
                int s1,
                float p0,
                float p1);

    // nearest interpolate
    // multiplies ne0 and ne1 by scale factor
    // used in stable-diffusion
    public static native ggml_tensor ggml_upscale(
                ggml_context ctx,
                ggml_tensor a,
                int scale_factor);

    // nearest interpolate
    // nearest interpolate to specified dimensions
    // used in tortoise.cpp
    public static native ggml_tensor ggml_upscale_ext(
                ggml_context ctx,
                ggml_tensor a,
                int ne0,
                int ne1,
                int ne2,
                int ne3);

    // pad each dimension with zeros: [x, ..., x] -> [x, ..., x, 0, ..., 0]
    public static native ggml_tensor ggml_pad(
                ggml_context ctx,
                ggml_tensor a,
                int p0,
                int p1,
                int p2,
                int p3);

    // pad each dimension with reflection: [a, b, c, d] -> [b, a, b, c, d, c]
    public static native ggml_tensor ggml_pad_reflect_1d(
                ggml_context ctx,
                ggml_tensor a,
                int p0,
                int p1);

    // Ref: https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/util.py#L151
    // timesteps: [N,]
    // return: [N, dim]
    public static native ggml_tensor ggml_timestep_embedding(
                ggml_context ctx,
                ggml_tensor timesteps,
                int dim,
                int max_period);

    // sort rows
    /** enum ggml_sort_order */
    public static final int
        GGML_SORT_ORDER_ASC = 0,
        GGML_SORT_ORDER_DESC = 1;

    public static native ggml_tensor ggml_argsort(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("ggml_sort_order") int order);

    public static native ggml_tensor ggml_arange(
                ggml_context ctx,
                float start,
                float stop,
                float step);

    // top k elements per row
    public static native ggml_tensor ggml_top_k(
                ggml_context ctx,
                ggml_tensor a,
                int k);

public static final int GGML_KQ_MASK_PAD = 64;

    // q:    [n_embd, n_batch,     n_head,    1]
    // k:    [n_embd, n_kv,        n_head_kv, 1]
    // v:    [n_embd, n_kv,        n_head_kv, 1] !! not transposed !!
    // mask: [n_kv,   n_batch_pad, 1,         1] !! n_batch_pad = GGML_PAD(n_batch, GGML_KQ_MASK_PAD) !!
    // res:  [n_embd, n_head,      n_batch,   1] !! permuted !!
    public static native ggml_tensor ggml_flash_attn_ext(
                ggml_context ctx,
                ggml_tensor q,
                ggml_tensor k,
                ggml_tensor v,
                ggml_tensor mask,
                float scale,
                float max_bias,
                float logit_softcap);

    public static native void ggml_flash_attn_ext_set_prec(
                ggml_tensor a,
                @Cast("ggml_prec") int prec);

    public static native @Cast("ggml_prec") int ggml_flash_attn_ext_get_prec(
                @Const ggml_tensor a);

    // TODO: needs to be adapted to ggml_flash_attn_ext
    public static native ggml_tensor ggml_flash_attn_back(
               ggml_context ctx,
               ggml_tensor q,
               ggml_tensor k,
               ggml_tensor v,
               ggml_tensor d,
               @Cast("bool") boolean masked);

    public static native ggml_tensor ggml_ssm_conv(
                ggml_context ctx,
                ggml_tensor sx,
                ggml_tensor c);

    public static native ggml_tensor ggml_ssm_scan(
                ggml_context ctx,
                ggml_tensor s,
                ggml_tensor x,
                ggml_tensor dt,
                ggml_tensor A,
                ggml_tensor B,
                ggml_tensor C);

    // partition into non-overlapping windows with padding if needed
    // example:
    // a:   768   64   64    1
    // w:    14
    // res: 768   14   14    25
    // used in sam
    public static native ggml_tensor ggml_win_part(
                ggml_context ctx,
                ggml_tensor a,
                int w);

    // reverse of ggml_win_part
    // used in sam
    public static native ggml_tensor ggml_win_unpart(
                ggml_context ctx,
                ggml_tensor a,
                int w0,
                int h0,
                int w);

    public static native ggml_tensor ggml_unary(
                ggml_context ctx,
                 ggml_tensor a,
                 @Cast("ggml_unary_op") int op);

    public static native ggml_tensor ggml_unary_inplace(
            ggml_context ctx,
            ggml_tensor a,
            @Cast("ggml_unary_op") int op);

    // used in sam
    public static native ggml_tensor ggml_get_rel_pos(
                ggml_context ctx,
                ggml_tensor a,
                int qh,
                int kh);

    // used in sam
    public static native ggml_tensor ggml_add_rel_pos(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor pw,
                ggml_tensor ph);

    public static native ggml_tensor ggml_add_rel_pos_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor pw,
                ggml_tensor ph);

    public static native ggml_tensor ggml_rwkv_wkv6(
                ggml_context ctx,
                ggml_tensor k,
                ggml_tensor v,
                ggml_tensor r,
                ggml_tensor tf,
                ggml_tensor td,
                ggml_tensor state);

    public static native ggml_tensor ggml_gated_linear_attn(
                ggml_context ctx,
                ggml_tensor k,
                ggml_tensor v,
                ggml_tensor q,
                ggml_tensor g,
                ggml_tensor state,
                float scale);
// Targeting ggml_unary_op_f32_t.java


// Targeting ggml_binary_op_f32_t.java


// Targeting ggml_custom1_op_f32_t.java


// Targeting ggml_custom2_op_f32_t.java


// Targeting ggml_custom3_op_f32_t.java



    public static native ggml_tensor ggml_map_unary_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_unary_op_f32_t fun);

    public static native ggml_tensor ggml_map_unary_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_unary_op_f32_t fun);

    public static native ggml_tensor ggml_map_binary_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_binary_op_f32_t fun);

    public static native ggml_tensor ggml_map_binary_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_binary_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom1_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_custom1_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom1_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_custom1_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom2_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_custom2_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom2_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_custom2_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom3_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                       ggml_custom3_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom3_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                       ggml_custom3_op_f32_t fun);
// Targeting ggml_custom1_op_t.java


// Targeting ggml_custom2_op_t.java


// Targeting ggml_custom3_op_t.java



public static final int GGML_N_TASKS_MAX = (-1);
    // n_tasks == GGML_N_TASKS_MAX means to use max number of tasks

    public static native ggml_tensor ggml_map_custom1(
                ggml_context ctx,
                ggml_tensor a,
                ggml_custom1_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom1_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_custom1_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom2(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_custom2_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom2_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_custom2_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom3(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                ggml_custom3_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom3_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                ggml_custom3_op_t fun,
                int n_tasks,
                Pointer userdata);

    // loss function

    public static native ggml_tensor ggml_cross_entropy_loss(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b); // labels

    public static native ggml_tensor ggml_cross_entropy_loss_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c); // gradients of cross_entropy_loss result

    // AdamW optimizer step
    // Paper: https://arxiv.org/pdf/1711.05101v3.pdf
    // PyTorch: https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html
    public static native ggml_tensor ggml_opt_step_adamw(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor grad,
                ggml_tensor m,
                ggml_tensor v,
                ggml_tensor adamw_params); // parameters such a the learning rate

    //
    // automatic differentiation
    //

    public static native void ggml_build_forward_expand(ggml_cgraph cgraph, ggml_tensor tensor);
    public static native void ggml_build_backward_expand(
            ggml_context ctx_static,
            ggml_context ctx_compute,
            ggml_cgraph cgraph,
            @Cast("bool") boolean accumulate); // whether or not gradients should be accumulated, requires static allocation of tensors in ctx_static

    // graph allocation in a context
    public static native ggml_cgraph ggml_new_graph(ggml_context ctx); // size = GGML_DEFAULT_GRAPH_SIZE, grads = false
    public static native ggml_cgraph ggml_new_graph_custom(ggml_context ctx, @Cast("size_t") long size, @Cast("bool") boolean grads);
    public static native ggml_cgraph ggml_graph_dup(ggml_context ctx, ggml_cgraph cgraph);
    public static native void ggml_graph_cpy(ggml_cgraph src, ggml_cgraph dst);
    public static native void ggml_graph_reset(ggml_cgraph cgraph); // set regular grads + optimizer momenta to 0, set loss grad to 1
    public static native void ggml_graph_clear(ggml_cgraph cgraph);

    public static native int ggml_graph_size(ggml_cgraph cgraph);
    public static native ggml_tensor ggml_graph_node(ggml_cgraph cgraph, int i); // if i < 0, returns nodes[n_nodes + i]
    public static native @Cast("ggml_tensor**") PointerPointer ggml_graph_nodes(ggml_cgraph cgraph);
    public static native int ggml_graph_n_nodes(ggml_cgraph cgraph);

    public static native void ggml_graph_add_node(ggml_cgraph cgraph, ggml_tensor tensor);

    public static native @Cast("size_t") long ggml_graph_overhead();
    public static native @Cast("size_t") long ggml_graph_overhead_custom(@Cast("size_t") long size, @Cast("bool") boolean grads);

    public static native ggml_tensor ggml_graph_get_tensor(@Const ggml_cgraph cgraph, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_graph_get_tensor(@Const ggml_cgraph cgraph, String name);
    public static native ggml_tensor ggml_graph_get_grad(@Const ggml_cgraph cgraph, @Const ggml_tensor node);
    public static native ggml_tensor ggml_graph_get_grad_acc(@Const ggml_cgraph cgraph, @Const ggml_tensor node);

    
    

    // print info and performance information for the graph
    public static native void ggml_graph_print(@Const ggml_cgraph cgraph);

    // dump the graph into a file using the dot format
    public static native void ggml_graph_dump_dot(@Const ggml_cgraph gb, @Const ggml_cgraph gf, @Cast("const char*") BytePointer filename);
    public static native void ggml_graph_dump_dot(@Const ggml_cgraph gb, @Const ggml_cgraph gf, String filename);
// Targeting ggml_log_callback.java



    // Set callback for all future logging events.
    // If this is not called, or NULL is supplied, everything is output on stderr.
    public static native void ggml_log_set(ggml_log_callback log_callback, Pointer user_data);

    public static native ggml_tensor ggml_set_zero(ggml_tensor tensor);

    //
    // quantization
    //

    // - ggml_quantize_init can be called multiple times with the same type
    //   it will only initialize the quantization tables for the first call or after ggml_quantize_free
    //   automatically called by ggml_quantize_chunk for convenience
    //
    // - ggml_quantize_free will free any memory allocated by ggml_quantize_init
    //   call this at the end of the program to avoid memory leaks
    //
    // note: these are thread-safe
    //
    public static native void ggml_quantize_init(@Cast("ggml_type") int type);
    public static native void ggml_quantize_free();

    // some quantization type cannot be used without an importance matrix
    public static native @Cast("bool") boolean ggml_quantize_requires_imatrix(@Cast("ggml_type") int type);

    // calls ggml_quantize_init internally (i.e. can allocate memory)
    public static native @Cast("size_t") long ggml_quantize_chunk(
                @Cast("ggml_type") int type,
                   @Const FloatPointer src,
                          Pointer dst,
                       @Cast("int64_t") long start,
                       @Cast("int64_t") long nrows,
                       @Cast("int64_t") long n_per_row,
                   @Const FloatPointer imatrix);
    public static native @Cast("size_t") long ggml_quantize_chunk(
                @Cast("ggml_type") int type,
                   @Const FloatBuffer src,
                          Pointer dst,
                       @Cast("int64_t") long start,
                       @Cast("int64_t") long nrows,
                       @Cast("int64_t") long n_per_row,
                   @Const FloatBuffer imatrix);
    public static native @Cast("size_t") long ggml_quantize_chunk(
                @Cast("ggml_type") int type,
                   @Const float[] src,
                          Pointer dst,
                       @Cast("int64_t") long start,
                       @Cast("int64_t") long nrows,
                       @Cast("int64_t") long n_per_row,
                   @Const float[] imatrix);

// #ifdef __cplusplus
    // restrict not standard in C++
// #    if defined(__GNUC__)
// #        define GGML_RESTRICT __restrict__
// #    elif defined(__clang__)
// #        define GGML_RESTRICT __restrict
// #    elif defined(_MSC_VER)
// #        define GGML_RESTRICT __restrict
// #    else
// #        define GGML_RESTRICT
// #    endif
// #else
// #    define GGML_RESTRICT restrict
// Targeting ggml_to_float_t.java


// Targeting ggml_from_float_t.java


// Targeting ggml_type_traits.java



    public static native @Const ggml_type_traits ggml_get_type_traits(@Cast("ggml_type") int type);

    // ggml threadpool
    // TODO: currently, only a few functions are in the base ggml API, while the rest are in the CPU backend
    // the goal should be to create an API that other backends can use move everything to the ggml base

    // scheduling priorities
    /** enum ggml_sched_priority */
    public static final int
        GGML_SCHED_PRIO_NORMAL = 0,
        GGML_SCHED_PRIO_MEDIUM = 1,
        GGML_SCHED_PRIO_HIGH = 2,
        GGML_SCHED_PRIO_REALTIME = 3;
// Targeting ggml_threadpool_params.java


// Targeting ggml_threadpool.java

     // forward declaration, see ggml.c

    public static native @ByVal ggml_threadpool_params ggml_threadpool_params_default(int n_threads);
    public static native void ggml_threadpool_params_init(ggml_threadpool_params p, int n_threads);
    public static native @Cast("bool") boolean ggml_threadpool_params_match(@Const ggml_threadpool_params p0, @Const ggml_threadpool_params p1);

// #ifdef  __cplusplus
// #endif


// Parsed from ggml-backend.h

// #pragma once

// #include "ggml.h"
// #include "ggml-alloc.h"

// #ifdef GGML_BACKEND_SHARED
// #    if defined(_WIN32) && !defined(__MINGW32__)
// #        ifdef GGML_BACKEND_BUILD
// #            define GGML_BACKEND_API __declspec(dllexport) extern
// #        else
// #            define GGML_BACKEND_API __declspec(dllimport) extern
// #        endif
// #    else
// #        define GGML_BACKEND_API __attribute__ ((visibility ("default"))) extern
// #    endif
// #else
// #    define GGML_BACKEND_API extern
// #endif

// #ifdef  __cplusplus
// Targeting ggml_backend_event.java


// Targeting ggml_backend_graph_plan_t.java


// Targeting ggml_backend_reg.java


// Targeting ggml_backend_device.java




    //
    // Backend buffer type
    //

    public static native @Cast("const char*") BytePointer ggml_backend_buft_name(ggml_backend_buffer_type buft);
    public static native ggml_backend_buffer ggml_backend_buft_alloc_buffer(ggml_backend_buffer_type buft, @Cast("size_t") long size);
    public static native @Cast("size_t") long ggml_backend_buft_get_alignment(ggml_backend_buffer_type buft);
    public static native @Cast("size_t") long ggml_backend_buft_get_max_size(ggml_backend_buffer_type buft);
    public static native @Cast("size_t") long ggml_backend_buft_get_alloc_size(ggml_backend_buffer_type buft, ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_backend_buft_is_host(ggml_backend_buffer_type buft);
    public static native ggml_backend_device ggml_backend_buft_get_device(ggml_backend_buffer_type buft);

    //
    // Backend buffer
    //

    /** enum ggml_backend_buffer_usage */
    public static final int
        GGML_BACKEND_BUFFER_USAGE_ANY = 0,
        GGML_BACKEND_BUFFER_USAGE_WEIGHTS = 1,
        GGML_BACKEND_BUFFER_USAGE_COMPUTE = 2;

    public static native @Cast("const char*") BytePointer ggml_backend_buffer_name(ggml_backend_buffer buffer);
    public static native void ggml_backend_buffer_free(ggml_backend_buffer buffer);
    public static native Pointer ggml_backend_buffer_get_base(ggml_backend_buffer buffer);
    public static native @Cast("size_t") long ggml_backend_buffer_get_size(ggml_backend_buffer buffer);
    public static native void ggml_backend_buffer_init_tensor(ggml_backend_buffer buffer, ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_backend_buffer_get_alignment(ggml_backend_buffer buffer);
    public static native @Cast("size_t") long ggml_backend_buffer_get_max_size(ggml_backend_buffer buffer);
    public static native @Cast("size_t") long ggml_backend_buffer_get_alloc_size(ggml_backend_buffer buffer, ggml_tensor tensor);
    public static native void ggml_backend_buffer_clear(ggml_backend_buffer buffer, @Cast("uint8_t") byte value);
    public static native @Cast("bool") boolean ggml_backend_buffer_is_host(ggml_backend_buffer buffer);
    public static native void ggml_backend_buffer_set_usage(ggml_backend_buffer buffer, @Cast("ggml_backend_buffer_usage") int usage);
    public static native @Cast("ggml_backend_buffer_usage") int ggml_backend_buffer_get_usage(ggml_backend_buffer buffer);
    public static native ggml_backend_buffer_type ggml_backend_buffer_get_type(ggml_backend_buffer buffer);
    public static native void ggml_backend_buffer_reset(ggml_backend_buffer buffer);

    // tensor copy between different backends
    public static native void ggml_backend_tensor_copy(ggml_tensor src, ggml_tensor dst);

    //
    // Backend (stream)
    //

    public static native @Cast("ggml_guid_t") byte ggml_backend_guid(ggml_backend backend);
    public static native @Cast("const char*") BytePointer ggml_backend_name(ggml_backend backend);
    public static native void ggml_backend_free(ggml_backend backend);

    public static native ggml_backend_buffer_type ggml_backend_get_default_buffer_type(ggml_backend backend);
    public static native ggml_backend_buffer ggml_backend_alloc_buffer(ggml_backend backend, @Cast("size_t") long size);
    public static native @Cast("size_t") long ggml_backend_get_alignment(ggml_backend backend);
    public static native @Cast("size_t") long ggml_backend_get_max_size(ggml_backend backend);

    public static native void ggml_backend_tensor_set_async(ggml_backend backend,       ggml_tensor tensor, @Const Pointer data, @Cast("size_t") long offset, @Cast("size_t") long size);
    public static native void ggml_backend_tensor_get_async(ggml_backend backend, @Const ggml_tensor tensor,       Pointer data, @Cast("size_t") long offset, @Cast("size_t") long size);

    // "offset" refers to the offset in tensor->data for setting/getting data
    public static native void ggml_backend_tensor_set(      ggml_tensor tensor, @Const Pointer data, @Cast("size_t") long offset, @Cast("size_t") long size);
    public static native void ggml_backend_tensor_get(@Const ggml_tensor tensor,       Pointer data, @Cast("size_t") long offset, @Cast("size_t") long size);
    public static native void ggml_backend_tensor_memset(   ggml_tensor tensor,     @Cast("uint8_t") byte value, @Cast("size_t") long offset, @Cast("size_t") long size);

    public static native void ggml_backend_synchronize(ggml_backend backend);

    public static native ggml_backend_graph_plan_t ggml_backend_graph_plan_create(ggml_backend backend, ggml_cgraph cgraph);
    public static native void ggml_backend_graph_plan_free(ggml_backend backend, ggml_backend_graph_plan_t plan);

    public static native @Cast("ggml_status") int ggml_backend_graph_plan_compute(ggml_backend backend, ggml_backend_graph_plan_t plan);
    public static native @Cast("ggml_status") int ggml_backend_graph_compute(ggml_backend backend, ggml_cgraph cgraph);
    public static native @Cast("ggml_status") int ggml_backend_graph_compute_async(ggml_backend backend, ggml_cgraph cgraph);

    // NOTE: will be removed, use device version instead
    public static native @Cast("bool") boolean ggml_backend_supports_op(ggml_backend backend, @Const ggml_tensor op);
    public static native @Cast("bool") boolean ggml_backend_supports_buft(ggml_backend backend, ggml_backend_buffer_type buft);
    public static native @Cast("bool") boolean ggml_backend_offload_op(ggml_backend backend, @Const ggml_tensor op);

    // asynchronous copy
    // the copy is performed after all the currently queued operations in backend_src
    // backend_dst will wait for the copy to complete before performing other operations
    // automatic fallback to sync copy if async is not supported
    public static native void ggml_backend_tensor_copy_async(ggml_backend backend_src, ggml_backend backend_dst, ggml_tensor src, ggml_tensor dst);

    public static native ggml_backend_device ggml_backend_get_device(ggml_backend backend);

    //
    // Events
    //

    public static native ggml_backend_event ggml_backend_event_new(ggml_backend_device device);
    public static native void ggml_backend_event_free(ggml_backend_event event);
    public static native void ggml_backend_event_record(ggml_backend_event event, ggml_backend backend);
    public static native void ggml_backend_event_synchronize(ggml_backend_event event);
    public static native void ggml_backend_event_wait(ggml_backend backend, ggml_backend_event event);

    //
    // Backend device
    //

    

    // functionality supported by the device

    // all the device properties

    public static native @Cast("const char*") BytePointer ggml_backend_dev_name(ggml_backend_device device);
    public static native @Cast("const char*") BytePointer ggml_backend_dev_description(ggml_backend_device device);
    public static native void ggml_backend_dev_memory(ggml_backend_device device, @Cast("size_t*") SizeTPointer _free, @Cast("size_t*") SizeTPointer total);
    
    public static native ggml_backend_reg ggml_backend_dev_backend_reg(ggml_backend_device device);
    public static native ggml_backend ggml_backend_dev_init(ggml_backend_device device, @Cast("const char*") BytePointer params);
    public static native ggml_backend ggml_backend_dev_init(ggml_backend_device device, String params);
    public static native ggml_backend_buffer_type ggml_backend_dev_buffer_type(ggml_backend_device device);
    public static native ggml_backend_buffer_type ggml_backend_dev_host_buffer_type(ggml_backend_device device);
    public static native ggml_backend_buffer ggml_backend_dev_buffer_from_host_ptr(ggml_backend_device device, Pointer ptr, @Cast("size_t") long size, @Cast("size_t") long max_tensor_size);

    public static native @Cast("bool") boolean ggml_backend_dev_supports_op(ggml_backend_device device, @Const ggml_tensor op);
    public static native @Cast("bool") boolean ggml_backend_dev_supports_buft(ggml_backend_device device, ggml_backend_buffer_type buft);
    public static native @Cast("bool") boolean ggml_backend_dev_offload_op(ggml_backend_device device, @Const ggml_tensor op);

    //
    // Backend (reg)
    //

    public static native @Cast("const char*") BytePointer ggml_backend_reg_name(ggml_backend_reg reg);
    public static native @Cast("size_t") long ggml_backend_reg_dev_count(ggml_backend_reg reg);
    public static native ggml_backend_device ggml_backend_reg_dev_get(ggml_backend_reg reg, @Cast("size_t") long index);
    public static native Pointer ggml_backend_reg_get_proc_address(ggml_backend_reg reg, @Cast("const char*") BytePointer name);
    public static native Pointer ggml_backend_reg_get_proc_address(ggml_backend_reg reg, String name);
// Targeting ggml_backend_split_buffer_type_t.java


// Targeting ggml_backend_set_n_threads_t.java


// Targeting ggml_backend_dev_get_extra_bufts_t.java


// Targeting ggml_backend_set_abort_callback_t.java


// Targeting ggml_backend_feature.java


// Targeting ggml_backend_get_features_t.java



    //
    // Backend registry
    //

    public static native void ggml_backend_device_register(ggml_backend_device device);

    // Backend (reg) enumeration
    public static native @Cast("size_t") long ggml_backend_reg_count();
    public static native ggml_backend_reg ggml_backend_reg_get(@Cast("size_t") long index);
    public static native ggml_backend_reg ggml_backend_reg_by_name(@Cast("const char*") BytePointer name);
    public static native ggml_backend_reg ggml_backend_reg_by_name(String name);

    // Device enumeration
    public static native @Cast("size_t") long ggml_backend_dev_count();
    public static native ggml_backend_device ggml_backend_dev_get(@Cast("size_t") long index);
    public static native ggml_backend_device ggml_backend_dev_by_name(@Cast("const char*") BytePointer name);
    public static native ggml_backend_device ggml_backend_dev_by_name(String name);

    // Direct backend (stream) initialization
    // = ggml_backend_dev_init(ggml_backend_dev_by_name(name), params)
    public static native ggml_backend ggml_backend_init_by_name(@Cast("const char*") BytePointer name, @Cast("const char*") BytePointer params);
    public static native ggml_backend ggml_backend_init_by_name(String name, String params);
    // = ggml_backend_dev_init(ggml_backend_dev_by_type(type), params)
    // = ggml_backend_dev_init(ggml_backend_dev_by_type(GPU) OR ggml_backend_dev_by_type(CPU), NULL)
    public static native ggml_backend ggml_backend_init_best();

    // Load a backend from a dynamic library and register it
    public static native ggml_backend_reg ggml_backend_load(@Cast("const char*") BytePointer path);
    public static native ggml_backend_reg ggml_backend_load(String path);
    // Unload a backend if loaded dynamically and unregister it
    public static native void ggml_backend_unload(ggml_backend_reg reg);
    // Load all known backends from dynamic libraries
    public static native void ggml_backend_load_all();
    public static native void ggml_backend_load_all_from_path(@Cast("const char*") BytePointer dir_path);
    public static native void ggml_backend_load_all_from_path(String dir_path);
// Targeting ggml_backend_sched.java


// Targeting ggml_backend_sched_eval_callback.java



    // Initialize a backend scheduler, backends with low index are given priority over backends with high index
    public static native ggml_backend_sched ggml_backend_sched_new(@Cast("ggml_backend**") PointerPointer backends, @Cast("ggml_backend_buffer_type**") PointerPointer bufts, int n_backends, @Cast("size_t") long graph_size, @Cast("bool") boolean parallel);
    public static native ggml_backend_sched ggml_backend_sched_new(@ByPtrPtr ggml_backend backends, @ByPtrPtr ggml_backend_buffer_type bufts, int n_backends, @Cast("size_t") long graph_size, @Cast("bool") boolean parallel);
    public static native void ggml_backend_sched_free(ggml_backend_sched sched);

    // Initialize backend buffers from a measure graph
    public static native @Cast("bool") boolean ggml_backend_sched_reserve(ggml_backend_sched sched, ggml_cgraph measure_graph); // returns success

    public static native int ggml_backend_sched_get_n_backends(ggml_backend_sched sched);
    public static native ggml_backend ggml_backend_sched_get_backend(ggml_backend_sched sched, int i);

    // Get the number of splits of the last graph
    public static native int ggml_backend_sched_get_n_splits(ggml_backend_sched sched);
    public static native int ggml_backend_sched_get_n_copies(ggml_backend_sched sched);

    public static native @Cast("size_t") long ggml_backend_sched_get_buffer_size(ggml_backend_sched sched, ggml_backend backend);

    public static native void ggml_backend_sched_set_tensor_backend(ggml_backend_sched sched, ggml_tensor node, ggml_backend backend);
    public static native ggml_backend ggml_backend_sched_get_tensor_backend(ggml_backend_sched sched, ggml_tensor node);

    // Allocate and compute graph on the backend scheduler
    public static native @Cast("bool") boolean ggml_backend_sched_alloc_graph(ggml_backend_sched sched, ggml_cgraph graph); // returns success
    public static native @Cast("ggml_status") int ggml_backend_sched_graph_compute(ggml_backend_sched sched, ggml_cgraph graph);
    public static native @Cast("ggml_status") int ggml_backend_sched_graph_compute_async(ggml_backend_sched sched, ggml_cgraph graph);
    public static native void ggml_backend_sched_synchronize(ggml_backend_sched sched);

    // Reset all assignments and allocators - must be called before changing the node backends or allocating a new graph.
    // This in effect deallocates all tensors that were previously allocated and leaves them with dangling pointers.
    // The correct way to use this API is to discard the deallocated tensors and create new ones.
    public static native void ggml_backend_sched_reset(ggml_backend_sched sched);

    // Set a callback to be called for each resulting node during graph compute
    public static native void ggml_backend_sched_set_eval_callback(ggml_backend_sched sched, ggml_backend_sched_eval_callback callback, Pointer user_data);

    //
    // Utils
    //

    // Copy a graph to a different backend
    
// Targeting ggml_backend_eval_callback.java



    // Compare the output of two backends
    public static native @Cast("bool") boolean ggml_backend_compare_graph_backend(ggml_backend backend1, ggml_backend backend2, ggml_cgraph graph, ggml_backend_eval_callback callback, Pointer user_data);

    // Tensor initialization
    public static native void ggml_backend_tensor_alloc(ggml_backend_buffer buffer, ggml_tensor tensor, Pointer addr);
    public static native void ggml_backend_view_init(ggml_tensor tensor);

    // CPU buffer types are always available
    public static native ggml_backend_buffer ggml_backend_cpu_buffer_from_ptr(Pointer ptr, @Cast("size_t") long size);
    public static native ggml_backend_buffer_type ggml_backend_cpu_buffer_type();

// #ifdef  __cplusplus
// #endif


// Parsed from ggml-cpu.h

// #pragma once

// #include "ggml.h"
// #include "ggml-backend.h"

// #ifdef  __cplusplus
// Targeting ggml_cplan.java



    // numa strategies
    /** enum ggml_numa_strategy */
    public static final int
        GGML_NUMA_STRATEGY_DISABLED   = 0,
        GGML_NUMA_STRATEGY_DISTRIBUTE = 1,
        GGML_NUMA_STRATEGY_ISOLATE    = 2,
        GGML_NUMA_STRATEGY_NUMACTL    = 3,
        GGML_NUMA_STRATEGY_MIRROR     = 4,
        GGML_NUMA_STRATEGY_COUNT = 5;

    public static native void ggml_numa_init(@Cast("ggml_numa_strategy") int numa); // call once for better performance on NUMA systems
    public static native @Cast("bool") boolean ggml_is_numa(); // true if init detected that system has >1 NUMA node

    public static native ggml_tensor ggml_new_i32(ggml_context ctx, int value);
    public static native ggml_tensor ggml_new_f32(ggml_context ctx, float value);

    public static native ggml_tensor ggml_set_i32(ggml_tensor tensor, int value);
    public static native ggml_tensor ggml_set_f32(ggml_tensor tensor, float value);

    public static native int ggml_get_i32_1d(@Const ggml_tensor tensor, int i);
    public static native void ggml_set_i32_1d(@Const ggml_tensor tensor, int i, int value);

    public static native int ggml_get_i32_nd(@Const ggml_tensor tensor, int i0, int i1, int i2, int i3);
    public static native void ggml_set_i32_nd(@Const ggml_tensor tensor, int i0, int i1, int i2, int i3, int value);

    public static native float ggml_get_f32_1d(@Const ggml_tensor tensor, int i);
    public static native void ggml_set_f32_1d(@Const ggml_tensor tensor, int i, float value);

    public static native float ggml_get_f32_nd(@Const ggml_tensor tensor, int i0, int i1, int i2, int i3);
    public static native void ggml_set_f32_nd(@Const ggml_tensor tensor, int i0, int i1, int i2, int i3, float value);

    public static native ggml_threadpool ggml_threadpool_new(ggml_threadpool_params params);
    public static native void ggml_threadpool_free(ggml_threadpool threadpool);
    
    public static native void ggml_threadpool_pause(ggml_threadpool threadpool);
    public static native void ggml_threadpool_resume(ggml_threadpool threadpool);

    // ggml_graph_plan() has to be called before ggml_graph_compute()
    // when plan.work_size > 0, caller must allocate memory for plan.work_data
    public static native @ByVal ggml_cplan ggml_graph_plan(
                      @Const ggml_cgraph cgraph,
                                           int n_threads,
                        ggml_threadpool threadpool );
    public static native @Cast("ggml_status") int ggml_graph_compute(ggml_cgraph cgraph, ggml_cplan cplan);

    // same as ggml_graph_compute() but the work data is allocated as a part of the context
    // note: the drawback of this API is that you must have ensured that the context has enough memory for the work data
    public static native @Cast("ggml_status") int ggml_graph_compute_with_ctx(ggml_context ctx, ggml_cgraph cgraph, int n_threads);

    //
    // system info
    //

    // x86
    public static native int ggml_cpu_has_sse3();
    public static native int ggml_cpu_has_ssse3();
    public static native int ggml_cpu_has_avx();
    public static native int ggml_cpu_has_avx_vnni();
    public static native int ggml_cpu_has_avx2();
    public static native int ggml_cpu_has_f16c();
    public static native int ggml_cpu_has_fma();
    public static native int ggml_cpu_has_avx512();
    public static native int ggml_cpu_has_avx512_vbmi();
    public static native int ggml_cpu_has_avx512_vnni();
    public static native int ggml_cpu_has_avx512_bf16();
    public static native int ggml_cpu_has_amx_int8();
    // ARM
    public static native int ggml_cpu_has_neon();
    public static native int ggml_cpu_has_arm_fma();
    public static native int ggml_cpu_has_fp16_va();
    public static native int ggml_cpu_has_dotprod();
    public static native int ggml_cpu_has_matmul_int8();
    public static native int ggml_cpu_has_sve();
    public static native int ggml_cpu_get_sve_cnt();  // sve vector length in bytes
    // other
    public static native int ggml_cpu_has_riscv_v();
    public static native int ggml_cpu_has_vsx();
    public static native int ggml_cpu_has_wasm_simd();
    public static native int ggml_cpu_has_llamafile();
// Targeting ggml_vec_dot_t.java


// Targeting ggml_type_traits_cpu.java



    public static native @Const ggml_type_traits_cpu ggml_get_type_traits_cpu(@Cast("ggml_type") int type);

    public static native void ggml_cpu_init();

    //
    // CPU backend
    //

    public static native ggml_backend ggml_backend_cpu_init();

    public static native @Cast("bool") boolean ggml_backend_is_cpu(ggml_backend backend);
    public static native void ggml_backend_cpu_set_n_threads(ggml_backend backend_cpu, int n_threads);
    public static native void ggml_backend_cpu_set_threadpool(ggml_backend backend_cpu, ggml_threadpool threadpool);
    public static native void ggml_backend_cpu_set_abort_callback(ggml_backend backend_cpu, ggml_abort_callback abort_callback, Pointer abort_callback_data);

    public static native ggml_backend_reg ggml_backend_cpu_reg();

// #ifdef __cplusplus
// #endif


}
