// Targeted by JavaCPP version 1.5.10: DO NOT EDIT THIS FILE

package org.swdc.llama.core;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import org.swdc.llama.core.ggml.*;
import static org.swdc.llama.core.ggml.GGML.*;

public class LLamaCore extends org.swdc.llama.config.LLamaConfigure {
    static { Loader.load(); }

// Parsed from llama.h

// #ifndef LLAMA_H
// #define LLAMA_H

// #include "ggml.h"
// #include "ggml-cpu.h"
// #include "ggml-backend.h"
// #include "ggml-opt.h"

// #include <stddef.h>
// #include <stdint.h>
// #include <stdio.h>
// #include <stdbool.h>

// #ifdef LLAMA_SHARED
// #    if defined(_WIN32) && !defined(__MINGW32__)
// #        ifdef LLAMA_BUILD
// #            define LLAMA_API __declspec(dllexport)
// #        else
// #            define LLAMA_API __declspec(dllimport)
// #        endif
// #    else
// #        define LLAMA_API __attribute__ ((visibility ("default")))
// #    endif
// #else
// #    define LLAMA_API
// #endif

// #ifdef __GNUC__
// #    define DEPRECATED(func, hint) func __attribute__((deprecated(hint)))
// #elif defined(_MSC_VER)
// #    define DEPRECATED(func, hint) __declspec(deprecated(hint)) func
// #else
// #    define DEPRECATED(func, hint) func
// #endif

public static final int LLAMA_DEFAULT_SEED = 0xFFFFFFFF;

public static final int LLAMA_TOKEN_NULL = -1;

public static final int LLAMA_FILE_MAGIC_GGLA = 0x67676c61; // 'ggla'
public static final int LLAMA_FILE_MAGIC_GGSN = 0x6767736e; // 'ggsn'
public static final int LLAMA_FILE_MAGIC_GGSQ = 0x67677371; // 'ggsq'

public static final int LLAMA_SESSION_MAGIC =   LLAMA_FILE_MAGIC_GGSN;
public static final int LLAMA_SESSION_VERSION = 9;

public static final int LLAMA_STATE_SEQ_MAGIC =   LLAMA_FILE_MAGIC_GGSQ;
public static final int LLAMA_STATE_SEQ_VERSION = 2;

// #ifdef __cplusplus
// Targeting llama_vocab.java


// Targeting llama_model.java


// Targeting llama_context.java


// Targeting llama_memory_i.java



    

    

    

    

    // model file types
    

    

    

    

    /** enum llama_flash_attn_type */
    public static final int
        LLAMA_FLASH_ATTN_TYPE_AUTO     = -1,
        LLAMA_FLASH_ATTN_TYPE_DISABLED = 0,
        LLAMA_FLASH_ATTN_TYPE_ENABLED  = 1;

    public static native @Cast("const char*") BytePointer llama_flash_attn_type_name(@Cast("llama_flash_attn_type") int flash_attn_type);

    
// Targeting llama_token_data.java


// Targeting llama_token_data_array.java


// Targeting llama_progress_callback.java


// Targeting llama_batch.java



    /** enum llama_model_kv_override_type */
    public static final int
        LLAMA_KV_OVERRIDE_TYPE_INT = 0,
        LLAMA_KV_OVERRIDE_TYPE_FLOAT = 1,
        LLAMA_KV_OVERRIDE_TYPE_BOOL = 2,
        LLAMA_KV_OVERRIDE_TYPE_STR = 3;
// Targeting llama_model_kv_override.java


// Targeting llama_model_tensor_buft_override.java


// Targeting llama_model_params.java


// Targeting llama_context_params.java


// Targeting llama_model_quantize_params.java


// Targeting llama_logit_bias.java


// Targeting llama_sampler_chain_params.java


// Targeting llama_chat_message.java


// Targeting llama_adapter_lora.java



    // Helpers for getting default parameters
    // TODO: update API to start accepting pointers to params structs (https://github.com/ggml-org/llama.cpp/discussions/9172)
    public static native @ByVal llama_model_params llama_model_default_params();
    public static native @ByVal llama_context_params llama_context_default_params();
    public static native @ByVal llama_sampler_chain_params llama_sampler_chain_default_params();
    public static native @ByVal llama_model_quantize_params llama_model_quantize_default_params();

    // Initialize the llama + ggml backend
    // If numa is true, use NUMA optimizations
    // Call once at the start of the program
    public static native void llama_backend_init();

    // Call once at the end of the program - currently only used for MPI
    public static native void llama_backend_free();

    //optional:
    public static native void llama_numa_init(@Cast("ggml_numa_strategy") int numa);

    // Optional: an auto threadpool gets created in ggml if not passed explicitly
    public static native void llama_attach_threadpool(
                llama_context ctx,
                   ggml_threadpool threadpool,
                   ggml_threadpool threadpool_batch);

    public static native void llama_detach_threadpool(llama_context ctx);

    public static native llama_model llama_load_model_from_file(
                                 @Cast("const char*") BytePointer path_model,
                  @ByVal llama_model_params params);
    public static native llama_model llama_load_model_from_file(
                                 String path_model,
                  @ByVal llama_model_params params);

    // Load the model from a file
    // If the file is split into multiple parts, the file name must follow this pattern: <name>-%05d-of-%05d.gguf
    // If the split file name does not follow this pattern, use llama_model_load_from_splits
    public static native llama_model llama_model_load_from_file(
                                 @Cast("const char*") BytePointer path_model,
                  @ByVal llama_model_params params);
    public static native llama_model llama_model_load_from_file(
                                 String path_model,
                  @ByVal llama_model_params params);

    // Load the model from multiple splits (support custom naming scheme)
    // The paths must be in the correct order
    public static native llama_model llama_model_load_from_splits(
                                 @Cast("const char**") PointerPointer paths,
                                     @Cast("size_t") long n_paths,
                  @ByVal llama_model_params params);
    public static native llama_model llama_model_load_from_splits(
                                 @Cast("const char**") @ByPtrPtr BytePointer paths,
                                     @Cast("size_t") long n_paths,
                  @ByVal llama_model_params params);
    public static native llama_model llama_model_load_from_splits(
                                 @Cast("const char**") @ByPtrPtr ByteBuffer paths,
                                     @Cast("size_t") long n_paths,
                  @ByVal llama_model_params params);
    public static native llama_model llama_model_load_from_splits(
                                 @Cast("const char**") @ByPtrPtr byte[] paths,
                                     @Cast("size_t") long n_paths,
                  @ByVal llama_model_params params);

    public static native void llama_model_save_to_file(
                @Const llama_model model,
                            @Cast("const char*") BytePointer path_model);
    public static native void llama_model_save_to_file(
                @Const llama_model model,
                            String path_model);

    public static native void llama_free_model(llama_model model);

    public static native void llama_model_free(llama_model model);

    public static native llama_context llama_init_from_model(
                         llama_model model,
                @ByVal llama_context_params params);

    public static native llama_context llama_new_context_with_model(
                         llama_model model,
                @ByVal llama_context_params params);

    // Frees all allocated memory
    public static native void llama_free(llama_context ctx);

    public static native @Cast("int64_t") long llama_time_us();

    public static native @Cast("size_t") long llama_max_devices();
    public static native @Cast("size_t") long llama_max_parallel_sequences();

    public static native @Cast("bool") boolean llama_supports_mmap();
    public static native @Cast("bool") boolean llama_supports_mlock();
    public static native @Cast("bool") boolean llama_supports_gpu_offload();
    public static native @Cast("bool") boolean llama_supports_rpc();

    public static native @Cast("uint32_t") int llama_n_ctx(@Const llama_context ctx);
    public static native @Cast("uint32_t") int llama_n_batch(@Const llama_context ctx);
    public static native @Cast("uint32_t") int llama_n_ubatch(@Const llama_context ctx);
    public static native @Cast("uint32_t") int llama_n_seq_max(@Const llama_context ctx);

    public static native int llama_n_ctx_train(@Const llama_model model);
    public static native int llama_n_embd(@Const llama_model model);
    public static native int llama_n_layer(@Const llama_model model);
    public static native int llama_n_head(@Const llama_model model);

    public static native int llama_n_vocab(@Const llama_vocab vocab);

    public static native @Const llama_model llama_get_model(@Const llama_context ctx);
    public static native llama_memory_i llama_get_memory(@Const llama_context ctx);
     // TODO: rename to llama_get_pooling_type

    public static native @Const llama_vocab llama_model_get_vocab(@Const llama_model model);

    public static native int llama_model_n_ctx_train(@Const llama_model model);
    public static native int llama_model_n_embd(@Const llama_model model);
    public static native int llama_model_n_layer(@Const llama_model model);
    public static native int llama_model_n_head(@Const llama_model model);
    public static native int llama_model_n_head_kv(@Const llama_model model);
    public static native int llama_model_n_swa(@Const llama_model model);

    // Get the model's RoPE frequency scaling factor
    public static native float llama_model_rope_freq_scale_train(@Const llama_model model);

    // Returns the number of classifier outputs (only valid for classifier models)
    // Undefined behavior for non-classifier models
    public static native @Cast("uint32_t") int llama_model_n_cls_out(@Const llama_model model);

    // Returns label of classifier output by index (<n_cls_out). Returns nullptr if no label provided
    public static native @Cast("const char*") BytePointer llama_model_cls_label(@Const llama_model model, @Cast("uint32_t") int i);

    

    public static native int llama_vocab_n_tokens(@Const llama_vocab vocab);

    // Functions to access the model's GGUF metadata scalar values
    // - The functions return the length of the string on success, or -1 on failure
    // - The output string is always null-terminated and cleared on failure
    // - When retrieving a string, an extra byte must be allocated to account for the null terminator
    // - GGUF array values are not supported by these functions

    // Get metadata value as a string by key name
    public static native int llama_model_meta_val_str(@Const llama_model model, @Cast("const char*") BytePointer key, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str(@Const llama_model model, String key, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str(@Const llama_model model, @Cast("const char*") BytePointer key, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str(@Const llama_model model, String key, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str(@Const llama_model model, @Cast("const char*") BytePointer key, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str(@Const llama_model model, String key, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);

    // Get the number of metadata key/value pairs
    public static native int llama_model_meta_count(@Const llama_model model);

    // Get metadata key name by index
    public static native int llama_model_meta_key_by_index(@Const llama_model model, int i, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_key_by_index(@Const llama_model model, int i, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_key_by_index(@Const llama_model model, int i, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);

    // Get metadata value as a string by index
    public static native int llama_model_meta_val_str_by_index(@Const llama_model model, int i, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str_by_index(@Const llama_model model, int i, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str_by_index(@Const llama_model model, int i, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);

    // Get a string describing the model type
    public static native int llama_model_desc(@Const llama_model model, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_desc(@Const llama_model model, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_desc(@Const llama_model model, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);

    // Returns the total size of all the tensors in the model in bytes
    public static native @Cast("uint64_t") long llama_model_size(@Const llama_model model);

    // Get the default chat template. Returns nullptr if not available
    // If name is NULL, returns the default chat template
    public static native @Cast("const char*") BytePointer llama_model_chat_template(@Const llama_model model, @Cast("const char*") BytePointer name);
    public static native String llama_model_chat_template(@Const llama_model model, String name);

    // Returns the total number of parameters in the model
    public static native @Cast("uint64_t") long llama_model_n_params(@Const llama_model model);

    // Returns true if the model contains an encoder that requires llama_encode() call
    public static native @Cast("bool") boolean llama_model_has_encoder(@Const llama_model model);

    // Returns true if the model contains a decoder that requires llama_decode() call
    public static native @Cast("bool") boolean llama_model_has_decoder(@Const llama_model model);

    // For encoder-decoder models, this function returns id of the token that must be provided
    // to the decoder to start generating output sequence. For other models, it returns -1.
    public static native @Cast("llama_token") int llama_model_decoder_start_token(@Const llama_model model);

    // Returns true if the model is recurrent (like Mamba, RWKV, etc.)
    public static native @Cast("bool") boolean llama_model_is_recurrent(@Const llama_model model);

    // Returns true if the model is diffusion-based (like LLaDA, Dream, etc.)
    public static native @Cast("bool") boolean llama_model_is_diffusion(@Const llama_model model);

    // Returns 0 on success
    public static native @Cast("uint32_t") int llama_model_quantize(
                @Cast("const char*") BytePointer fname_inp,
                @Cast("const char*") BytePointer fname_out,
                @Const llama_model_quantize_params params);
    public static native @Cast("uint32_t") int llama_model_quantize(
                String fname_inp,
                String fname_out,
                @Const llama_model_quantize_params params);

    //
    // Adapters
    //

    // Load a LoRA adapter from file
    public static native llama_adapter_lora llama_adapter_lora_init(
                llama_model model,
                @Cast("const char*") BytePointer path_lora);
    public static native llama_adapter_lora llama_adapter_lora_init(
                llama_model model,
                String path_lora);

    // Functions to access the adapter's GGUF metadata scalar values
    // - The functions return the length of the string on success, or -1 on failure
    // - The output string is always null-terminated and cleared on failure
    // - When retrieving a string, an extra byte must be allocated to account for the null terminator
    // - GGUF array values are not supported by these functions

    // Get metadata value as a string by key name
    public static native int llama_adapter_meta_val_str(@Const llama_adapter_lora adapter, @Cast("const char*") BytePointer key, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_adapter_meta_val_str(@Const llama_adapter_lora adapter, String key, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_adapter_meta_val_str(@Const llama_adapter_lora adapter, @Cast("const char*") BytePointer key, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);
    public static native int llama_adapter_meta_val_str(@Const llama_adapter_lora adapter, String key, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_adapter_meta_val_str(@Const llama_adapter_lora adapter, @Cast("const char*") BytePointer key, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_adapter_meta_val_str(@Const llama_adapter_lora adapter, String key, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);

    // Get the number of metadata key/value pairs
    public static native int llama_adapter_meta_count(@Const llama_adapter_lora adapter);

    // Get metadata key name by index
    public static native int llama_adapter_meta_key_by_index(@Const llama_adapter_lora adapter, int i, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_adapter_meta_key_by_index(@Const llama_adapter_lora adapter, int i, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_adapter_meta_key_by_index(@Const llama_adapter_lora adapter, int i, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);

    // Get metadata value as a string by index
    public static native int llama_adapter_meta_val_str_by_index(@Const llama_adapter_lora adapter, int i, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_adapter_meta_val_str_by_index(@Const llama_adapter_lora adapter, int i, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_adapter_meta_val_str_by_index(@Const llama_adapter_lora adapter, int i, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);

    // Manually free a LoRA adapter
    // Note: loaded adapters will be free when the associated model is deleted
    public static native void llama_adapter_lora_free(llama_adapter_lora adapter);

    // Get the invocation tokens if the current lora is an alora
    public static native @Cast("uint64_t") long llama_adapter_get_alora_n_invocation_tokens(@Const llama_adapter_lora adapter);
    public static native @Cast("const llama_token*") IntPointer llama_adapter_get_alora_invocation_tokens(@Const llama_adapter_lora adapter);

    // The following functions operate on a llama_context, hence the naming: llama_verb_...

    // Add a loaded LoRA adapter to given context
    // This will not modify model's weight
    public static native int llama_set_adapter_lora(
                llama_context ctx,
                llama_adapter_lora adapter,
                float scale);

    // Remove a specific LoRA adapter from given context
    // Return -1 if the adapter is not present in the context
    public static native int llama_rm_adapter_lora(
                llama_context ctx,
                llama_adapter_lora adapter);

    // Remove all LoRA adapters from given context
    public static native void llama_clear_adapter_lora(llama_context ctx);

    // Apply a loaded control vector to a llama_context, or if data is NULL, clear
    // the currently loaded vector.
    // n_embd should be the size of a single layer's control, and data should point
    // to an n_embd x n_layers buffer starting from layer 1.
    // il_start and il_end are the layer range the vector should apply to (both inclusive)
    // See llama_control_vector_load in common to load a control vector.
    public static native int llama_apply_adapter_cvec(
                llama_context ctx,
                         @Const FloatPointer data,
                              @Cast("size_t") long len,
                             int n_embd,
                             int il_start,
                             int il_end);
    public static native int llama_apply_adapter_cvec(
                llama_context ctx,
                         @Const FloatBuffer data,
                              @Cast("size_t") long len,
                             int n_embd,
                             int il_start,
                             int il_end);
    public static native int llama_apply_adapter_cvec(
                llama_context ctx,
                         @Const float[] data,
                              @Cast("size_t") long len,
                             int n_embd,
                             int il_start,
                             int il_end);

    //
    // Memory
    //

    // Clear the memory contents
    // If data == true, the data buffers will also be cleared together with the metadata
    public static native void llama_memory_clear(
                llama_memory_i mem,
                          @Cast("bool") boolean data);

    // Removes all tokens that belong to the specified sequence and have positions in [p0, p1)
    // Returns false if a partial sequence cannot be removed. Removing a whole sequence never fails
    // seq_id < 0 : match any sequence
    // p0 < 0     : [0,  p1]
    // p1 < 0     : [p0, inf)
    public static native @Cast("bool") boolean llama_memory_seq_rm(
                llama_memory_i mem,
                  @Cast("llama_seq_id") int seq_id,
                     @Cast("llama_pos") int p0,
                     @Cast("llama_pos") int p1);

    // Copy all tokens that belong to the specified sequence to another sequence
    // p0 < 0 : [0,  p1]
    // p1 < 0 : [p0, inf)
    public static native void llama_memory_seq_cp(
                llama_memory_i mem,
                  @Cast("llama_seq_id") int seq_id_src,
                  @Cast("llama_seq_id") int seq_id_dst,
                     @Cast("llama_pos") int p0,
                     @Cast("llama_pos") int p1);

    // Removes all tokens that do not belong to the specified sequence
    public static native void llama_memory_seq_keep(
                llama_memory_i mem,
                  @Cast("llama_seq_id") int seq_id);

    // Adds relative position "delta" to all tokens that belong to the specified sequence and have positions in [p0, p1)
    // p0 < 0 : [0,  p1]
    // p1 < 0 : [p0, inf)
    public static native void llama_memory_seq_add(
                llama_memory_i mem,
                  @Cast("llama_seq_id") int seq_id,
                     @Cast("llama_pos") int p0,
                     @Cast("llama_pos") int p1,
                     @Cast("llama_pos") int delta);

    // Integer division of the positions by factor of `d > 1`
    // p0 < 0 : [0,  p1]
    // p1 < 0 : [p0, inf)
    public static native void llama_memory_seq_div(
                llama_memory_i mem,
                  @Cast("llama_seq_id") int seq_id,
                     @Cast("llama_pos") int p0,
                     @Cast("llama_pos") int p1,
                           int d);

    // Returns the smallest position present in the memory for the specified sequence
    // This is typically non-zero only for SWA caches
    // Note that all positions in the range [pos_min, pos_max] are guaranteed to be present in the memory
    // Return -1 if the sequence is empty
    public static native @Cast("llama_pos") int llama_memory_seq_pos_min(
                llama_memory_i mem,
                  @Cast("llama_seq_id") int seq_id);

    // Returns the largest position present in the memory for the specified sequence
    // Note that all positions in the range [pos_min, pos_max] are guaranteed to be present in the memory
    // Return -1 if the sequence is empty
    public static native @Cast("llama_pos") int llama_memory_seq_pos_max(
                llama_memory_i mem,
                  @Cast("llama_seq_id") int seq_id);

    // Check if the memory supports shifting
    public static native @Cast("bool") boolean llama_memory_can_shift(llama_memory_i mem);

    //
    // State / sessions
    //

    // Returns the *actual* size in bytes of the state
    // (logits, embedding and memory)
    // Only use when saving the state, not when restoring it, otherwise the size may be too small.
    public static native @Cast("size_t") long llama_state_get_size(llama_context ctx);
    public static native @Cast("size_t") long llama_get_state_size(llama_context ctx);

    // Copies the state to the specified destination address.
    // Destination needs to have allocated enough memory.
    // Returns the number of bytes copied
    public static native @Cast("size_t") long llama_state_get_data(
                llama_context ctx,
                             @Cast("uint8_t*") BytePointer dst,
                              @Cast("size_t") long size);
    public static native @Cast("size_t") long llama_state_get_data(
                llama_context ctx,
                             @Cast("uint8_t*") ByteBuffer dst,
                              @Cast("size_t") long size);
    public static native @Cast("size_t") long llama_state_get_data(
                llama_context ctx,
                             @Cast("uint8_t*") byte[] dst,
                              @Cast("size_t") long size);
    public static native @Cast("size_t") long llama_copy_state_data(
                llama_context ctx,
                             @Cast("uint8_t*") BytePointer dst);
    public static native @Cast("size_t") long llama_copy_state_data(
                llama_context ctx,
                             @Cast("uint8_t*") ByteBuffer dst);
    public static native @Cast("size_t") long llama_copy_state_data(
                llama_context ctx,
                             @Cast("uint8_t*") byte[] dst);

    // Set the state reading from the specified address
    // Returns the number of bytes read
    public static native @Cast("size_t") long llama_state_set_data(
                llama_context ctx,
                       @Cast("const uint8_t*") BytePointer src,
                              @Cast("size_t") long size);
    public static native @Cast("size_t") long llama_state_set_data(
                llama_context ctx,
                       @Cast("const uint8_t*") ByteBuffer src,
                              @Cast("size_t") long size);
    public static native @Cast("size_t") long llama_state_set_data(
                llama_context ctx,
                       @Cast("const uint8_t*") byte[] src,
                              @Cast("size_t") long size);
    public static native @Cast("size_t") long llama_set_state_data(
                llama_context ctx,
                       @Cast("const uint8_t*") BytePointer src);
    public static native @Cast("size_t") long llama_set_state_data(
                llama_context ctx,
                       @Cast("const uint8_t*") ByteBuffer src);
    public static native @Cast("size_t") long llama_set_state_data(
                llama_context ctx,
                       @Cast("const uint8_t*") byte[] src);

    // Save/load session file
    public static native @Cast("bool") boolean llama_state_load_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                         @Cast("llama_token*") IntPointer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_state_load_file(
                llama_context ctx,
                          String path_session,
                         @Cast("llama_token*") IntBuffer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_state_load_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                         @Cast("llama_token*") int[] tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_state_load_file(
                llama_context ctx,
                          String path_session,
                         @Cast("llama_token*") IntPointer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_state_load_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                         @Cast("llama_token*") IntBuffer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_state_load_file(
                llama_context ctx,
                          String path_session,
                         @Cast("llama_token*") int[] tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                         @Cast("llama_token*") IntPointer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(
                llama_context ctx,
                          String path_session,
                         @Cast("llama_token*") IntBuffer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                         @Cast("llama_token*") int[] tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(
                llama_context ctx,
                          String path_session,
                         @Cast("llama_token*") IntPointer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                         @Cast("llama_token*") IntBuffer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(
                llama_context ctx,
                          String path_session,
                         @Cast("llama_token*") int[] tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);

    public static native @Cast("bool") boolean llama_state_save_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                   @Cast("const llama_token*") IntPointer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_state_save_file(
                llama_context ctx,
                          String path_session,
                   @Cast("const llama_token*") IntBuffer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_state_save_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                   @Cast("const llama_token*") int[] tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_state_save_file(
                llama_context ctx,
                          String path_session,
                   @Cast("const llama_token*") IntPointer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_state_save_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                   @Cast("const llama_token*") IntBuffer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_state_save_file(
                llama_context ctx,
                          String path_session,
                   @Cast("const llama_token*") int[] tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                   @Cast("const llama_token*") IntPointer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(
                llama_context ctx,
                          String path_session,
                   @Cast("const llama_token*") IntBuffer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                   @Cast("const llama_token*") int[] tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(
                llama_context ctx,
                          String path_session,
                   @Cast("const llama_token*") IntPointer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                   @Cast("const llama_token*") IntBuffer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(
                llama_context ctx,
                          String path_session,
                   @Cast("const llama_token*") int[] tokens,
                              @Cast("size_t") long n_token_count);

    // Get the exact size needed to copy the state of a single sequence
    public static native @Cast("size_t") long llama_state_seq_get_size(
                llama_context ctx,
                        @Cast("llama_seq_id") int seq_id);

    // Copy the state of a single sequence into the specified buffer
    public static native @Cast("size_t") long llama_state_seq_get_data(
                llama_context ctx,
                             @Cast("uint8_t*") BytePointer dst,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int seq_id);
    public static native @Cast("size_t") long llama_state_seq_get_data(
                llama_context ctx,
                             @Cast("uint8_t*") ByteBuffer dst,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int seq_id);
    public static native @Cast("size_t") long llama_state_seq_get_data(
                llama_context ctx,
                             @Cast("uint8_t*") byte[] dst,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int seq_id);

    // Copy the sequence data (originally copied with `llama_state_seq_get_data`) into the specified sequence
    // Returns:
    //  - Positive: Ok
    //  - Zero: Failed to load
    public static native @Cast("size_t") long llama_state_seq_set_data(
                llama_context ctx,
                       @Cast("const uint8_t*") BytePointer src,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int dest_seq_id);
    public static native @Cast("size_t") long llama_state_seq_set_data(
                llama_context ctx,
                       @Cast("const uint8_t*") ByteBuffer src,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int dest_seq_id);
    public static native @Cast("size_t") long llama_state_seq_set_data(
                llama_context ctx,
                       @Cast("const uint8_t*") byte[] src,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int dest_seq_id);

    public static native @Cast("size_t") long llama_state_seq_save_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer filepath,
                        @Cast("llama_seq_id") int seq_id,
                   @Cast("const llama_token*") IntPointer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("size_t") long llama_state_seq_save_file(
                llama_context ctx,
                          String filepath,
                        @Cast("llama_seq_id") int seq_id,
                   @Cast("const llama_token*") IntBuffer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("size_t") long llama_state_seq_save_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer filepath,
                        @Cast("llama_seq_id") int seq_id,
                   @Cast("const llama_token*") int[] tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("size_t") long llama_state_seq_save_file(
                llama_context ctx,
                          String filepath,
                        @Cast("llama_seq_id") int seq_id,
                   @Cast("const llama_token*") IntPointer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("size_t") long llama_state_seq_save_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer filepath,
                        @Cast("llama_seq_id") int seq_id,
                   @Cast("const llama_token*") IntBuffer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("size_t") long llama_state_seq_save_file(
                llama_context ctx,
                          String filepath,
                        @Cast("llama_seq_id") int seq_id,
                   @Cast("const llama_token*") int[] tokens,
                              @Cast("size_t") long n_token_count);

    public static native @Cast("size_t") long llama_state_seq_load_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer filepath,
                        @Cast("llama_seq_id") int dest_seq_id,
                         @Cast("llama_token*") IntPointer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("size_t") long llama_state_seq_load_file(
                llama_context ctx,
                          String filepath,
                        @Cast("llama_seq_id") int dest_seq_id,
                         @Cast("llama_token*") IntBuffer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("size_t") long llama_state_seq_load_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer filepath,
                        @Cast("llama_seq_id") int dest_seq_id,
                         @Cast("llama_token*") int[] tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("size_t") long llama_state_seq_load_file(
                llama_context ctx,
                          String filepath,
                        @Cast("llama_seq_id") int dest_seq_id,
                         @Cast("llama_token*") IntPointer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("size_t") long llama_state_seq_load_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer filepath,
                        @Cast("llama_seq_id") int dest_seq_id,
                         @Cast("llama_token*") IntBuffer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("size_t") long llama_state_seq_load_file(
                llama_context ctx,
                          String filepath,
                        @Cast("llama_seq_id") int dest_seq_id,
                         @Cast("llama_token*") int[] tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);

public static final int LLAMA_STATE_SEQ_FLAGS_SWA_ONLY = 1;

    public static native @Cast("size_t") long llama_state_seq_get_size_ext(
                llama_context ctx,
                        @Cast("llama_seq_id") int seq_id,
               @Cast("llama_state_seq_flags") int flags);

    public static native @Cast("size_t") long llama_state_seq_get_data_ext(
                llama_context ctx,
                             @Cast("uint8_t*") BytePointer dst,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int seq_id,
               @Cast("llama_state_seq_flags") int flags);
    public static native @Cast("size_t") long llama_state_seq_get_data_ext(
                llama_context ctx,
                             @Cast("uint8_t*") ByteBuffer dst,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int seq_id,
               @Cast("llama_state_seq_flags") int flags);
    public static native @Cast("size_t") long llama_state_seq_get_data_ext(
                llama_context ctx,
                             @Cast("uint8_t*") byte[] dst,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int seq_id,
               @Cast("llama_state_seq_flags") int flags);

    public static native @Cast("size_t") long llama_state_seq_set_data_ext(
                llama_context ctx,
                       @Cast("const uint8_t*") BytePointer src,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int dest_seq_id,
               @Cast("llama_state_seq_flags") int flags);
    public static native @Cast("size_t") long llama_state_seq_set_data_ext(
                llama_context ctx,
                       @Cast("const uint8_t*") ByteBuffer src,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int dest_seq_id,
               @Cast("llama_state_seq_flags") int flags);
    public static native @Cast("size_t") long llama_state_seq_set_data_ext(
                llama_context ctx,
                       @Cast("const uint8_t*") byte[] src,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int dest_seq_id,
               @Cast("llama_state_seq_flags") int flags);

    //
    // Decoding
    //

    // Return batch for single sequence of tokens
    // The sequence ID will be fixed to 0
    // The position of the tokens will be tracked automatically by llama_decode
    //
    // NOTE: this is a helper function to facilitate transition to the new batch API - avoid using it
    //
    public static native @ByVal llama_batch llama_batch_get_one(
                      @Cast("llama_token*") IntPointer tokens,
                          int n_tokens);
    public static native @ByVal llama_batch llama_batch_get_one(
                      @Cast("llama_token*") IntBuffer tokens,
                          int n_tokens);
    public static native @ByVal llama_batch llama_batch_get_one(
                      @Cast("llama_token*") int[] tokens,
                          int n_tokens);

    // Allocates a batch of tokens on the heap that can hold a maximum of n_tokens
    // Each token can be assigned up to n_seq_max sequence ids
    // The batch has to be freed with llama_batch_free()
    // If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)
    // Otherwise, llama_batch.token will be allocated to store n_tokens llama_token
    // The rest of the llama_batch members are allocated with size n_tokens
    // All members are left uninitialized
    public static native @ByVal llama_batch llama_batch_init(
                int n_tokens,
                int embd,
                int n_seq_max);

    // Frees a batch of tokens allocated with llama_batch_init()
    public static native void llama_batch_free(@ByVal llama_batch batch);

    // Process a batch of tokens.
    // In contrast to llama_decode() - this call does not use KV cache.
    // For encode-decoder contexts, processes the batch using the encoder.
    // Can store the encoder output internally for later use by the decoder's cross-attention layers.
    //   0 - success
    // < 0 - error. the memory state is restored to the state before this call
    public static native int llama_encode(
                llama_context ctx,
                  @ByVal llama_batch batch);

    // Process a batch of tokens.
    // Requires the context to have a memory.
    // For encode-decoder contexts, processes the batch using the decoder.
    // Positive return values does not mean a fatal error, but rather a warning.
    // Upon fatal-error or abort, the ubatches that managed to be been processed will remain in the memory state of the context
    //   To handle this correctly, query the memory state using llama_memory_seq_pos_min() and llama_memory_seq_pos_max()
    // Upon other return values, the memory state is restored to the state before this call
    //    0 - success
    //    1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)
    //    2 - aborted     (processed ubatches will remain in the context's memory)
    //   -1 - invalid input batch
    // < -1 - fatal error (processed ubatches will remain in the context's memory)
    public static native int llama_decode(
                llama_context ctx,
                  @ByVal llama_batch batch);

    // Set the number of threads used for decoding
    // n_threads is the number of threads used for generation (single token)
    // n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)
    public static native void llama_set_n_threads(llama_context ctx, int n_threads, int n_threads_batch);

    // Get the number of threads used for generation of a single token.
    public static native int llama_n_threads(llama_context ctx);

    // Get the number of threads used for prompt and batch processing (multiple token).
    public static native int llama_n_threads_batch(llama_context ctx);

    // Set whether the context outputs embeddings or not
    // TODO: rename to avoid confusion with llama_get_embeddings()
    public static native void llama_set_embeddings(llama_context ctx, @Cast("bool") boolean embeddings);

    // Set whether to use causal attention or not
    // If set to true, the model will only attend to the past tokens
    public static native void llama_set_causal_attn(llama_context ctx, @Cast("bool") boolean causal_attn);

    // Set whether the model is in warmup mode or not
    // If true, all model tensors are activated during llama_decode() to load and cache their weights.
    public static native void llama_set_warmup(llama_context ctx, @Cast("bool") boolean warmup);

    // Set abort callback
    public static native void llama_set_abort_callback(llama_context ctx, ggml_abort_callback abort_callback, Pointer abort_callback_data);

    // Wait until all computations are finished
    // This is automatically done when using one of the functions below to obtain the computation results
    // and is not necessary to call it explicitly in most cases
    public static native void llama_synchronize(llama_context ctx);

    // Token logits obtained from the last call to llama_decode()
    // The logits for which llama_batch.logits[i] != 0 are stored contiguously
    // in the order they have appeared in the batch.
    // Rows: number of tokens for which llama_batch.logits[i] != 0
    // Cols: n_vocab
    // TODO: deprecate in favor of llama_get_logits_ith() (ref: https://github.com/ggml-org/llama.cpp/pull/14853#issuecomment-3113143522)
    public static native FloatPointer llama_get_logits(llama_context ctx);

    // Logits for the ith token. For positive indices, Equivalent to:
    // llama_get_logits(ctx) + ctx->output_ids[i]*n_vocab
    // Negative indicies can be used to access logits in reverse order, -1 is the last logit.
    // returns NULL for invalid ids.
    public static native FloatPointer llama_get_logits_ith(llama_context ctx, int i);

    // Get all output token embeddings.
    // when pooling_type == LLAMA_POOLING_TYPE_NONE or when using a generative model,
    // the embeddings for which llama_batch.logits[i] != 0 are stored contiguously
    // in the order they have appeared in the batch.
    // shape: [n_outputs*n_embd]
    // Otherwise, returns NULL.
    // TODO: deprecate in favor of llama_get_embeddings_ith() (ref: https://github.com/ggml-org/llama.cpp/pull/14853#issuecomment-3113143522)
    public static native FloatPointer llama_get_embeddings(llama_context ctx);

    // Get the embeddings for the ith token. For positive indices, Equivalent to:
    // llama_get_embeddings(ctx) + ctx->output_ids[i]*n_embd
    // Negative indicies can be used to access embeddings in reverse order, -1 is the last embedding.
    // shape: [n_embd] (1-dimensional)
    // returns NULL for invalid ids.
    public static native FloatPointer llama_get_embeddings_ith(llama_context ctx, int i);

    // Get the embeddings for a sequence id
    // Returns NULL if pooling_type is LLAMA_POOLING_TYPE_NONE
    // when pooling_type == LLAMA_POOLING_TYPE_RANK, returns float[n_cls_out] with the rank(s) of the sequence
    // otherwise: float[n_embd] (1-dimensional)
    public static native FloatPointer llama_get_embeddings_seq(llama_context ctx, @Cast("llama_seq_id") int seq_id);

    //
    // Vocab
    //

    public static native @Cast("const char*") BytePointer llama_vocab_get_text(@Const llama_vocab vocab, @Cast("llama_token") int token);

    public static native float llama_vocab_get_score(@Const llama_vocab vocab, @Cast("llama_token") int token);

    // Check if the token is supposed to end generation (end-of-generation, eg. EOS, EOT, etc.)
    public static native @Cast("bool") boolean llama_vocab_is_eog(@Const llama_vocab vocab, @Cast("llama_token") int token);

    // Identify if Token Id is a control token or a render-able token
    public static native @Cast("bool") boolean llama_vocab_is_control(@Const llama_vocab vocab, @Cast("llama_token") int token);

    // Special tokens
    public static native @Cast("llama_token") int llama_vocab_bos(@Const llama_vocab vocab); // beginning-of-sentence
    public static native @Cast("llama_token") int llama_vocab_eos(@Const llama_vocab vocab); // end-of-sentence
    public static native @Cast("llama_token") int llama_vocab_eot(@Const llama_vocab vocab); // end-of-turn
    public static native @Cast("llama_token") int llama_vocab_sep(@Const llama_vocab vocab); // sentence separator
    public static native @Cast("llama_token") int llama_vocab_nl(@Const llama_vocab vocab); // next-line
    public static native @Cast("llama_token") int llama_vocab_pad(@Const llama_vocab vocab); // padding
    public static native @Cast("llama_token") int llama_vocab_mask(@Const llama_vocab vocab); // mask

    public static native @Cast("bool") boolean llama_vocab_get_add_bos(@Const llama_vocab vocab);
    public static native @Cast("bool") boolean llama_vocab_get_add_eos(@Const llama_vocab vocab);
    public static native @Cast("bool") boolean llama_vocab_get_add_sep(@Const llama_vocab vocab);

    public static native @Cast("llama_token") int llama_vocab_fim_pre(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_vocab_fim_suf(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_vocab_fim_mid(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_vocab_fim_pad(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_vocab_fim_rep(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_vocab_fim_sep(@Const llama_vocab vocab);

    public static native @Cast("const char*") BytePointer llama_token_get_text(@Const llama_vocab vocab, @Cast("llama_token") int token);
    public static native float llama_token_get_score(@Const llama_vocab vocab, @Cast("llama_token") int token);
    public static native @Cast("bool") boolean llama_token_is_eog(@Const llama_vocab vocab, @Cast("llama_token") int token);
    public static native @Cast("bool") boolean llama_token_is_control(@Const llama_vocab vocab, @Cast("llama_token") int token);
    public static native @Cast("llama_token") int llama_token_bos(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_eos(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_eot(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_cls(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_sep(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_nl(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_pad(@Const llama_vocab vocab);
    public static native @Cast("bool") boolean llama_add_bos_token(@Const llama_vocab vocab);
    public static native @Cast("bool") boolean llama_add_eos_token(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_fim_pre(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_fim_suf(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_fim_mid(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_fim_pad(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_fim_rep(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_fim_sep(@Const llama_vocab vocab);

    // CLS is equivalent to BOS
    public static native @Cast("llama_token") int llama_vocab_cls(@Const llama_vocab vocab);

    //
    // Tokenization
    //
    // The API is thread-safe.
    //

    /** \details Convert the provided text into tokens.
     *  @param tokens The tokens pointer must be large enough to hold the resulting tokens.
     *  @return Returns the number of tokens on success, no more than n_tokens_max
     *  @return Returns a negative number on failure - the number of tokens that would have been returned
     *  @return Returns INT32_MIN on overflow (e.g., tokenization result size exceeds int32_t limit)
     *  @param add_special Allow to add BOS and EOS tokens if model is configured to do so.
     *  @param parse_special Allow tokenizing special and/or control tokens which otherwise are not exposed and treated
     *                       as plaintext. Does not insert a leading space. */
    public static native int llama_tokenize(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer text,
                             int text_len,
                         @Cast("llama_token*") IntPointer tokens,
                             int n_tokens_max,
                                @Cast("bool") boolean add_special,
                                @Cast("bool") boolean parse_special);
    public static native int llama_tokenize(
            @Const llama_vocab vocab,
                          String text,
                             int text_len,
                         @Cast("llama_token*") IntBuffer tokens,
                             int n_tokens_max,
                                @Cast("bool") boolean add_special,
                                @Cast("bool") boolean parse_special);
    public static native int llama_tokenize(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer text,
                             int text_len,
                         @Cast("llama_token*") int[] tokens,
                             int n_tokens_max,
                                @Cast("bool") boolean add_special,
                                @Cast("bool") boolean parse_special);
    public static native int llama_tokenize(
            @Const llama_vocab vocab,
                          String text,
                             int text_len,
                         @Cast("llama_token*") IntPointer tokens,
                             int n_tokens_max,
                                @Cast("bool") boolean add_special,
                                @Cast("bool") boolean parse_special);
    public static native int llama_tokenize(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer text,
                             int text_len,
                         @Cast("llama_token*") IntBuffer tokens,
                             int n_tokens_max,
                                @Cast("bool") boolean add_special,
                                @Cast("bool") boolean parse_special);
    public static native int llama_tokenize(
            @Const llama_vocab vocab,
                          String text,
                             int text_len,
                         @Cast("llama_token*") int[] tokens,
                             int n_tokens_max,
                                @Cast("bool") boolean add_special,
                                @Cast("bool") boolean parse_special);

    // Token Id -> Piece.
    // Uses the vocabulary in the provided context.
    // Does not write null terminator to the buffer.
    // User can skip up to 'lstrip' leading spaces before copying (useful when encoding/decoding multiple tokens with 'add_space_prefix')
    // @param special If true, special tokens are rendered in the output.
    public static native int llama_token_to_piece(
                  @Const llama_vocab vocab,
                               @Cast("llama_token") int token,
                                      @Cast("char*") BytePointer buf,
                                   int length,
                                   int lstrip,
                                      @Cast("bool") boolean special);
    public static native int llama_token_to_piece(
                  @Const llama_vocab vocab,
                               @Cast("llama_token") int token,
                                      @Cast("char*") ByteBuffer buf,
                                   int length,
                                   int lstrip,
                                      @Cast("bool") boolean special);
    public static native int llama_token_to_piece(
                  @Const llama_vocab vocab,
                               @Cast("llama_token") int token,
                                      @Cast("char*") byte[] buf,
                                   int length,
                                   int lstrip,
                                      @Cast("bool") boolean special);

    /** \details Convert the provided tokens into text (inverse of llama_tokenize()).
     *  @param text The char pointer must be large enough to hold the resulting text.
     *  @return Returns the number of chars/bytes on success, no more than text_len_max.
     *  @return Returns a negative number on failure - the number of chars/bytes that would have been returned.
     *  @param remove_special Allow to remove BOS and EOS tokens if model is configured to do so.
     *  @param unparse_special If true, special tokens are rendered in the output. */
    public static native int llama_detokenize(
            @Const llama_vocab vocab,
                   @Cast("const llama_token*") IntPointer tokens,
                             int n_tokens,
                                @Cast("char*") BytePointer text,
                             int text_len_max,
                                @Cast("bool") boolean remove_special,
                                @Cast("bool") boolean unparse_special);
    public static native int llama_detokenize(
            @Const llama_vocab vocab,
                   @Cast("const llama_token*") IntBuffer tokens,
                             int n_tokens,
                                @Cast("char*") ByteBuffer text,
                             int text_len_max,
                                @Cast("bool") boolean remove_special,
                                @Cast("bool") boolean unparse_special);
    public static native int llama_detokenize(
            @Const llama_vocab vocab,
                   @Cast("const llama_token*") int[] tokens,
                             int n_tokens,
                                @Cast("char*") byte[] text,
                             int text_len_max,
                                @Cast("bool") boolean remove_special,
                                @Cast("bool") boolean unparse_special);

    //
    // Chat templates
    //

    /** Apply chat template. Inspired by hf apply_chat_template() on python.
     *  Both "model" and "custom_template" are optional, but at least one is required. "custom_template" has higher precedence than "model"
     *  NOTE: This function does not use a jinja parser. It only support a pre-defined list of template. See more: https://github.com/ggml-org/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template
     *  @param tmpl A Jinja template to use for this chat. If this is nullptr, the models default chat template will be used instead.
     *  @param chat Pointer to a list of multiple llama_chat_message
     *  @param n_msg Number of llama_chat_message in this chat
     *  @param add_ass Whether to end the prompt with the token(s) that indicate the start of an assistant message.
     *  @param buf A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)
     *  @param length The size of the allocated buffer
     *  @return The total number of bytes of the formatted prompt. If is it larger than the size of buffer, you may need to re-alloc it and then re-apply the template. */
    public static native int llama_chat_apply_template(
                                @Cast("const char*") BytePointer tmpl,
           @Const llama_chat_message chat,
                                    @Cast("size_t") long n_msg,
                                      @Cast("bool") boolean add_ass,
                                      @Cast("char*") BytePointer buf,
                                   int length);
    public static native int llama_chat_apply_template(
                                String tmpl,
           @Const llama_chat_message chat,
                                    @Cast("size_t") long n_msg,
                                      @Cast("bool") boolean add_ass,
                                      @Cast("char*") ByteBuffer buf,
                                   int length);
    public static native int llama_chat_apply_template(
                                @Cast("const char*") BytePointer tmpl,
           @Const llama_chat_message chat,
                                    @Cast("size_t") long n_msg,
                                      @Cast("bool") boolean add_ass,
                                      @Cast("char*") byte[] buf,
                                   int length);
    public static native int llama_chat_apply_template(
                                String tmpl,
           @Const llama_chat_message chat,
                                    @Cast("size_t") long n_msg,
                                      @Cast("bool") boolean add_ass,
                                      @Cast("char*") BytePointer buf,
                                   int length);
    public static native int llama_chat_apply_template(
                                @Cast("const char*") BytePointer tmpl,
           @Const llama_chat_message chat,
                                    @Cast("size_t") long n_msg,
                                      @Cast("bool") boolean add_ass,
                                      @Cast("char*") ByteBuffer buf,
                                   int length);
    public static native int llama_chat_apply_template(
                                String tmpl,
           @Const llama_chat_message chat,
                                    @Cast("size_t") long n_msg,
                                      @Cast("bool") boolean add_ass,
                                      @Cast("char*") byte[] buf,
                                   int length);

    // Get list of built-in chat templates
    public static native int llama_chat_builtin_templates(@Cast("const char**") PointerPointer output, @Cast("size_t") long len);
    public static native int llama_chat_builtin_templates(@Cast("const char**") @ByPtrPtr BytePointer output, @Cast("size_t") long len);
    public static native int llama_chat_builtin_templates(@Cast("const char**") @ByPtrPtr ByteBuffer output, @Cast("size_t") long len);
    public static native int llama_chat_builtin_templates(@Cast("const char**") @ByPtrPtr byte[] output, @Cast("size_t") long len);
// Targeting llama_sampler_context_t.java


// Targeting llama_sampler_i.java


// Targeting llama_sampler.java



    // mirror of llama_sampler_i:
    public static native llama_sampler llama_sampler_init(@Const llama_sampler_i iface, llama_sampler_context_t ctx);
    public static native @Cast("const char*") BytePointer llama_sampler_name(@Const llama_sampler smpl);
    public static native void llama_sampler_accept(      llama_sampler smpl, @Cast("llama_token") int token);
    public static native void llama_sampler_apply(      llama_sampler smpl, llama_token_data_array cur_p);
    public static native void llama_sampler_reset(      llama_sampler smpl);
    public static native llama_sampler llama_sampler_clone(@Const llama_sampler smpl);
    // important: do not free if the sampler has been added to a llama_sampler_chain (via llama_sampler_chain_add)
    public static native void llama_sampler_free(      llama_sampler smpl);

    // llama_sampler_chain
    // a type of llama_sampler that can chain multiple samplers one after another

    public static native llama_sampler llama_sampler_chain_init(@ByVal llama_sampler_chain_params params);

    // important: takes ownership of the sampler object and will free it when llama_sampler_free is called
    public static native void llama_sampler_chain_add(      llama_sampler chain, llama_sampler smpl);
    public static native llama_sampler llama_sampler_chain_get(@Const llama_sampler chain, int i);
    public static native int llama_sampler_chain_n(@Const llama_sampler chain);

    // after removing a sampler, the chain will no longer own it, and it will not be freed when the chain is freed
    public static native llama_sampler llama_sampler_chain_remove(   llama_sampler chain, int i);

    // available samplers:

    public static native llama_sampler llama_sampler_init_greedy();
    public static native llama_sampler llama_sampler_init_dist(@Cast("uint32_t") int seed);

    /** \details Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
     *  Setting k <= 0 makes this a noop */
    public static native llama_sampler llama_sampler_init_top_k(int k);

    /** \details Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751 */
    public static native llama_sampler llama_sampler_init_top_p(float p, @Cast("size_t") long min_keep);

    /** \details Minimum P sampling as described in https://github.com/ggml-org/llama.cpp/pull/3841 */
    public static native llama_sampler llama_sampler_init_min_p(float p, @Cast("size_t") long min_keep);

    /** \details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666. */
    public static native llama_sampler llama_sampler_init_typical(float p, @Cast("size_t") long min_keep);

    /** #details Updates the logits l_i{@code  = l_i/t. When t <= 0.0f, the maximum logit is kept at it's original value, the rest are set to -inf */
    public static native llama_sampler llama_sampler_init_temp(float t);

    /** \details Dynamic temperature implementation (a.k.a. entropy) described in the paper https://arxiv.org/abs/2309.02772. */
    public static native llama_sampler llama_sampler_init_temp_ext(float t, float delta, float exponent);

    /** \details XTC sampler as described in https://github.com/oobabooga/text-generation-webui/pull/6335 */
    public static native llama_sampler llama_sampler_init_xtc(float p, float t,     @Cast("size_t") long min_keep, @Cast("uint32_t") int seed);

    /** \details Top n sigma sampling as described in academic paper "Top-n: Not All Logits Are You Need" https://arxiv.org/pdf/2411.07641 */
    public static native llama_sampler llama_sampler_init_top_n_sigma(float n);

    /** \details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
     *  @param candidates A vector of {@code llama_token_data} containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
     *  @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
     *  @param eta The learning rate used to update {@code mu} based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause {@code mu} to be updated more quickly, while a smaller learning rate will result in slower updates.
     *  @param m The number of tokens considered in the estimation of {@code s_hat}. This is an arbitrary value that is used to calculate {@code s_hat}, which in turn helps to calculate the value of {@code k}. In the paper, they use {@code m = 100}, but you can experiment with different values to see how it affects the performance of the algorithm.
     *  @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy ({@code 2 * tau}) and is updated in the algorithm based on the error between the target and observed surprisal. */
    public static native llama_sampler llama_sampler_init_mirostat(
                                 int n_vocab,
                                @Cast("uint32_t") int seed,
                                   float tau,
                                   float eta,
                                 int m);

    /** \details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
     *  @param candidates A vector of {@code llama_token_data} containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
     *  @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
     *  @param eta The learning rate used to update {@code mu} based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause {@code mu} to be updated more quickly, while a smaller learning rate will result in slower updates.
     *  @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy ({@code 2 * tau}) and is updated in the algorithm based on the error between the target and observed surprisal. */
    public static native llama_sampler llama_sampler_init_mirostat_v2(
                                @Cast("uint32_t") int seed,
                                   float tau,
                                   float eta);

    /** \details Intializes a GBNF grammar, see grammars/README.md for details.
     *  @param vocab The vocabulary that this grammar will be used with.
     *  @param grammar_str The production rules for the grammar, encoded as a string. Returns an empty grammar if empty. Returns NULL if parsing of grammar_str fails.
     *  @param grammar_root The name of the start symbol for the grammar. */
    public static native llama_sampler llama_sampler_init_grammar(
                @Const llama_vocab vocab,
                              @Cast("const char*") BytePointer grammar_str,
                              @Cast("const char*") BytePointer grammar_root);
    public static native llama_sampler llama_sampler_init_grammar(
                @Const llama_vocab vocab,
                              String grammar_str,
                              String grammar_root);

    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              @Cast("const char*") BytePointer grammar_str,
                              @Cast("const char*") BytePointer grammar_root,
                             @Cast("const char**") PointerPointer trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") IntPointer trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              @Cast("const char*") BytePointer grammar_str,
                              @Cast("const char*") BytePointer grammar_root,
                             @Cast("const char**") @ByPtrPtr BytePointer trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") IntPointer trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              String grammar_str,
                              String grammar_root,
                             @Cast("const char**") @ByPtrPtr ByteBuffer trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") IntBuffer trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              @Cast("const char*") BytePointer grammar_str,
                              @Cast("const char*") BytePointer grammar_root,
                             @Cast("const char**") @ByPtrPtr byte[] trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") int[] trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              String grammar_str,
                              String grammar_root,
                             @Cast("const char**") @ByPtrPtr BytePointer trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") IntPointer trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              @Cast("const char*") BytePointer grammar_str,
                              @Cast("const char*") BytePointer grammar_root,
                             @Cast("const char**") @ByPtrPtr ByteBuffer trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") IntBuffer trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              String grammar_str,
                              String grammar_root,
                             @Cast("const char**") @ByPtrPtr byte[] trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") int[] trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);


    /** \details Lazy grammar sampler, introduced in https://github.com/ggml-org/llama.cpp/pull/9639
     *  @param trigger_patterns A list of patterns that will trigger the grammar sampler. Pattern will be matched from the start of the generation output, and grammar sampler will be fed content starting from its first match group.
     *  @param trigger_tokens A list of tokens that will trigger the grammar sampler. Grammar sampler will be fed content starting from the trigger token included. */
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer grammar_str,
                          @Cast("const char*") BytePointer grammar_root,
                         @Cast("const char**") PointerPointer trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") IntPointer trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer grammar_str,
                          @Cast("const char*") BytePointer grammar_root,
                         @Cast("const char**") @ByPtrPtr BytePointer trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") IntPointer trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          String grammar_str,
                          String grammar_root,
                         @Cast("const char**") @ByPtrPtr ByteBuffer trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") IntBuffer trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer grammar_str,
                          @Cast("const char*") BytePointer grammar_root,
                         @Cast("const char**") @ByPtrPtr byte[] trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") int[] trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          String grammar_str,
                          String grammar_root,
                         @Cast("const char**") @ByPtrPtr BytePointer trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") IntPointer trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer grammar_str,
                          @Cast("const char*") BytePointer grammar_root,
                         @Cast("const char**") @ByPtrPtr ByteBuffer trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") IntBuffer trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          String grammar_str,
                          String grammar_root,
                         @Cast("const char**") @ByPtrPtr byte[] trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") int[] trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);


    /** NOTE: Avoid using on the full vocabulary as searching for repeated tokens can become slow. For example, apply top-k or top-p sampling first. */
    public static native llama_sampler llama_sampler_init_penalties(
                                 int penalty_last_n,
                                   float penalty_repeat,
                                   float penalty_freq,
                                   float penalty_present); // 0.0 = disabled

    /**  \details DRY sampler, designed by p-e-w, as described in: https://github.com/oobabooga/text-generation-webui/pull/5677, porting Koboldcpp implementation authored by pi6am: https://github.com/LostRuins/koboldcpp/pull/982 */
    public static native llama_sampler llama_sampler_init_dry(
                @Const llama_vocab vocab,
                                 int n_ctx_train,
                                   float dry_multiplier,
                                   float dry_base,
                                 int dry_allowed_length,
                                 int dry_penalty_last_n,
                              @Cast("const char**") PointerPointer seq_breakers,
                                  @Cast("size_t") long num_breakers);
    public static native llama_sampler llama_sampler_init_dry(
                @Const llama_vocab vocab,
                                 int n_ctx_train,
                                   float dry_multiplier,
                                   float dry_base,
                                 int dry_allowed_length,
                                 int dry_penalty_last_n,
                              @Cast("const char**") @ByPtrPtr BytePointer seq_breakers,
                                  @Cast("size_t") long num_breakers);
    public static native llama_sampler llama_sampler_init_dry(
                @Const llama_vocab vocab,
                                 int n_ctx_train,
                                   float dry_multiplier,
                                   float dry_base,
                                 int dry_allowed_length,
                                 int dry_penalty_last_n,
                              @Cast("const char**") @ByPtrPtr ByteBuffer seq_breakers,
                                  @Cast("size_t") long num_breakers);
    public static native llama_sampler llama_sampler_init_dry(
                @Const llama_vocab vocab,
                                 int n_ctx_train,
                                   float dry_multiplier,
                                   float dry_base,
                                 int dry_allowed_length,
                                 int dry_penalty_last_n,
                              @Cast("const char**") @ByPtrPtr byte[] seq_breakers,
                                  @Cast("size_t") long num_breakers);

    public static native llama_sampler llama_sampler_init_logit_bias(
                                 int n_vocab,
                                 int n_logit_bias,
                  @Const llama_logit_bias logit_bias);

    // this sampler is meant to be used for fill-in-the-middle infilling
    // it's supposed to be used after top_k + top_p sampling
    //
    // 1. if the sum of the EOG probs times the number of candidates is higher than the sum of the other probs -> pick EOG
    // 2. combine probs of tokens that have the same prefix
    //
    // example:
    //
    // - before:
    //   "hel":   0.5
    //   "hell":  0.2
    //   "hello": 0.1
    //   "dummy": 0.1
    //
    // - after:
    //   "hel":   0.8
    //   "dummy": 0.1
    //
    // 3. discard non-EOG tokens with low prob
    // 4. if no tokens are left -> pick EOT
    //
    public static native llama_sampler llama_sampler_init_infill(@Const llama_vocab vocab);

    // Returns the seed used by the sampler if applicable, LLAMA_DEFAULT_SEED otherwise
    public static native @Cast("uint32_t") int llama_sampler_get_seed(@Const llama_sampler smpl);

    /** \details Sample and accept a token from the idx-th output of the last evaluation */
    //
    // Shorthand for:
    //    const auto * logits = llama_get_logits_ith(ctx, idx);
    //    llama_token_data_array cur_p = { ... init from logits ... };
    //    llama_sampler_apply(smpl, &cur_p);
    //    auto token = cur_p.data[cur_p.selected].id;
    //    llama_sampler_accept(smpl, token);
    //    return token;
    // Returns the sampled token
    public static native @Cast("llama_token") int llama_sampler_sample(llama_sampler smpl, llama_context ctx, int idx);

    // TODO: extend in the future
    //LLAMA_API void llama_decode_with_sampler(struct llama_context * ctx, struct llama_sampler * smpl, struct llama_batch batch, ...);

    //
    // Model split
    //

    /** \details Build a split GGUF final path for this chunk.
     *           llama_split_path(split_path, sizeof(split_path), "/models/ggml-model-q4_0", 2, 4) => split_path = "/models/ggml-model-q4_0-00002-of-00004.gguf" */
    //  Returns the split_path length.
    public static native int llama_split_path(@Cast("char*") BytePointer split_path, @Cast("size_t") long maxlen, @Cast("const char*") BytePointer path_prefix, int split_no, int split_count);
    public static native int llama_split_path(@Cast("char*") ByteBuffer split_path, @Cast("size_t") long maxlen, String path_prefix, int split_no, int split_count);
    public static native int llama_split_path(@Cast("char*") byte[] split_path, @Cast("size_t") long maxlen, @Cast("const char*") BytePointer path_prefix, int split_no, int split_count);
    public static native int llama_split_path(@Cast("char*") BytePointer split_path, @Cast("size_t") long maxlen, String path_prefix, int split_no, int split_count);
    public static native int llama_split_path(@Cast("char*") ByteBuffer split_path, @Cast("size_t") long maxlen, @Cast("const char*") BytePointer path_prefix, int split_no, int split_count);
    public static native int llama_split_path(@Cast("char*") byte[] split_path, @Cast("size_t") long maxlen, String path_prefix, int split_no, int split_count);

    /** \details Extract the path prefix from the split_path if and only if the split_no and split_count match.
     *           llama_split_prefix(split_prefix, 64, "/models/ggml-model-q4_0-00002-of-00004.gguf", 2, 4) => split_prefix = "/models/ggml-model-q4_0" */
    //  Returns the split_prefix length.
    public static native int llama_split_prefix(@Cast("char*") BytePointer split_prefix, @Cast("size_t") long maxlen, @Cast("const char*") BytePointer split_path, int split_no, int split_count);
    public static native int llama_split_prefix(@Cast("char*") ByteBuffer split_prefix, @Cast("size_t") long maxlen, String split_path, int split_no, int split_count);
    public static native int llama_split_prefix(@Cast("char*") byte[] split_prefix, @Cast("size_t") long maxlen, @Cast("const char*") BytePointer split_path, int split_no, int split_count);
    public static native int llama_split_prefix(@Cast("char*") BytePointer split_prefix, @Cast("size_t") long maxlen, String split_path, int split_no, int split_count);
    public static native int llama_split_prefix(@Cast("char*") ByteBuffer split_prefix, @Cast("size_t") long maxlen, @Cast("const char*") BytePointer split_path, int split_no, int split_count);
    public static native int llama_split_prefix(@Cast("char*") byte[] split_prefix, @Cast("size_t") long maxlen, String split_path, int split_no, int split_count);

    // Print system information
    public static native @Cast("const char*") BytePointer llama_print_system_info();

    // Set callback for all future logging events.
    // If this is not called, or NULL is supplied, everything is output on stderr.
    public static native void llama_log_set(ggml_log_callback log_callback, Pointer user_data);
// Targeting llama_perf_context_data.java


// Targeting llama_perf_sampler_data.java



    public static native @ByVal llama_perf_context_data llama_perf_context(@Const llama_context ctx);
    public static native void llama_perf_context_print(@Const llama_context ctx);
    public static native void llama_perf_context_reset(      llama_context ctx);

    // NOTE: the following work only with samplers constructed via llama_sampler_chain_init
    public static native @ByVal llama_perf_sampler_data llama_perf_sampler(@Const llama_sampler chain);
    public static native void llama_perf_sampler_print(@Const llama_sampler chain);
    public static native void llama_perf_sampler_reset(      llama_sampler chain);

    // print a breakdown of per-device memory use via LLAMA_LOG:
    public static native void llama_memory_breakdown_print(@Const llama_context ctx);
// Targeting llama_opt_param_filter.java



    // always returns true
    public static native @Cast("bool") boolean llama_opt_param_filter_all(@Const ggml_tensor tensor, Pointer userdata);
// Targeting llama_opt_params.java



    public static native void llama_opt_init(llama_context lctx, llama_model model, @ByVal llama_opt_params lopt_params);

    

// #ifdef __cplusplus
// #endif

// #endif // LLAMA_H


}
